[
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/",
	"title": "Báo cáo thực tập",
	"tags": [],
	"description": "",
	"content": "Báo cáo thực tập Thông tin sinh viên: Họ và tên: Trang Hoàng Khang\nSố điện thoại: 0837525888\nEmail: khangthse182228@fpt.edu.vn\nTrường: Đại học FPT TP.HCM\nNgành: Trí tuệ nhân tạo\nMSSV SE182228\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 08/09/2025 đến ngày 08/12/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Cách một khách hàng giảm 28% tổng chi phí sở hữu (TCO) cho việc lưu trữ với Amazon FSx for NetApp ONTAP bởi Sachin Bawse và Vishnu Vashist | vào ngày 08 tháng 9 năm 2025 | trong chuyên mục Advanced (300), Amazon FSx for NetApp ONTAP, Best Practices, Healthcare, Technical How-to | Permalink\nCác tổ chức có nhiều chi nhánh thường gặp thách thức lớn trong việc quản lý hệ thống tệp phân tán, đặc biệt khi sử dụng hạ tầng tại chỗ (on-premises) truyền thống. Việc duy trì khả năng chia sẻ tệp mượt mà giữa các vị trí địa lý khác nhau, đồng thời đảm bảo tính bảo mật, hiệu quả trong quản lý dữ liệu và xác thực người dùng đáng tin cậy, ngày càng trở nên phức tạp trong bối cảnh số hóa hiện nay.\nAmazon FSx for NetApp ONTAP giải quyết những thách thức này bằng cách cung cấp một giải pháp cloud-native được quản lý hoàn toàn, mang lại khả năng lưu trữ tệp hiệu năng cao với tích hợp sẵn tính năng sao chép dữ liệu (replication), đồng bộ tự động và bộ nhớ đệm thông minh (intelligent caching).\nTrong bài viết này, nhóm tác giả chia sẻ cách một khách hàng đã triển khai FSx for ONTAP với cấu hình Multi-AZ, sử dụng NetApp FlexCache cho bộ nhớ đệm cục bộ và thực hiện di chuyển dữ liệu bằng SnapMirror. Trong quá trình này, họ thực hiện các bài kiểm tra hiệu năng, phân tích các đánh đổi trong thiết kế kiến trúc và đưa ra so sánh chi phí — cho thấy mức giảm 28% chi phí sở hữu tổng thể (TCO) so với giải pháp tại chỗ truyền thống.\nTổng quan giải pháp Kiến trúc cốt lõi của giải pháp xoay quanh việc triển khai Amazon FSx for NetApp ONTAP Multi-AZ cluster trải rộng trên hai Vùng sẵn sàng (Availability Zones), nhằm đảm bảo tính sẵn sàng cao và khả năng chịu lỗi.\nCác chi nhánh kết nối tới FSxN thông qua các kết nối VPN bảo mật, truy cập dữ liệu thông qua ONTAP FlexCache volumes được triển khai tại môi trường on-prem hoặc VMware. Các bộ nhớ đệm này giúp giảm băng thông và cải thiện tốc độ phản hồi nhờ phục vụ dữ liệu được truy cập thường xuyên ngay tại chỗ.\nViệc di chuyển dữ liệu được thực hiện bằng NetApp SnapMirror, đảm bảo sao chép dữ liệu nhất quán từ kho lưu trữ on-prem lên AWS.\nHình 1: Hệ thống tệp phân tán với kiến trúc Amazon FSx hoặc NetApp ONTAP. Các lớp giao tiếp và bộ nhớ đệm bao gồm: • Chi nhánh địa phương → FlexCache volumes (phục vụ dữ liệu nóng) • FlexCache → FSx ONTAP origin cluster trên các AZ • SnapMirror → sao chép dữ liệu lên FSx, duy trì hiệu quả lưu trữ\nKhách hàng cũng tiến hành so sánh hiệu năng: khi truyền một tệp 50 MB, ONTAP Select cache kết nối FSx đạt 26,09 giây, so với 24,20 giây trên máy chủ tệp cục bộ, cho thấy hiệu năng gần tương đương với lưu trữ tại chỗ.\nCân nhắc về độ trễ và ghi lại FlexCache Tính năng ghi lại (write-back) của FlexCache đặc biệt hữu ích khi độ trễ (latency) giữa bộ nhớ đệm chi nhánh và cụm gốc vượt quá 8 ms. Trong điều kiện đó, bộ nhớ đệm giúp cải thiện hiệu năng ghi dữ liệu đáng kể.\nMột số yêu cầu thiết kế cần lưu ý: • CPU \u0026amp; RAM: Mỗi nút trong cụm gốc cần ít nhất 128 GB RAM và khoảng 20 vCPUs để xử lý tải write-back. • Phiên bản ONTAP: Cụm gốc và cụm cache phải chạy ONTAP 9.15.1 trở lên để hỗ trợ write-back. • Giấy phép (Licensing): FlexCache (bao gồm cả write-back) đã được tích hợp sẵn, không cần mua thêm license. • Cluster peering: Cụm gốc và cụm cache cần được kết nối ngang hàng (peered), các server virtual machines SVMs cũng cần được vserver-peered với FlexCache bật.\nNhững nguyên tắc này giúp đảm bảo bộ nhớ đệm hoạt động ổn định trong các mô hình chi nhánh phân tán.\nGiám sát và khả năng hiển thị Để có cái nhìn chi tiết hơn về hiệu suất và hoạt động của FSx \u0026amp; ONTAP ngoài dữ liệu mặc định trên Amazon CloudWatch, giải pháp sử dụng NetApp Harvest kết hợp với Grafana. Công cụ Harvest thu thập các chỉ số như hiệu năng, tỷ lệ cache hit, mức độ hiệu quả lưu trữ, và thống kê cấp volume, giúp quản trị viên có khả năng theo dõi chi tiết và chủ động tối ưu hóa.\nƯớc tính TCO so với on-premises Để định lượng lợi ích về chi phí, khách hàng đã mô hình hóa một kịch bản lai (hybrid) để so sánh giữa FSx for ONTAP (Multi-AZ, cấu hình SSD + capacity tier tỉ lệ khoảng 30/70, throughput ~256 MB/s) với hệ thống tại chỗ tương đương. Kết quả cho thấy FSx for ONTAP giúp giảm ước tính 28% tổng chi phí sở hữu (TCO) so với giải pháp tại chỗ, nhờ giảm chi phí vận hành, tăng hiệu quả lưu trữ, và đơn giản hóa hạ tầng quản lý.\nKết Luận Amazon FSx for NetApp ONTAP là một giải pháp mạnh mẽ cho các hệ thống tệp phân tán, đặc biệt phù hợp với mô hình nhiều chi nhánh.\nThông qua triển khai Multi-AZ, FlexCache để lưu đệm cục bộ, SnapMirror để di chuyển dữ liệu, cùng hệ thống giám sát chi tiết, kiến trúc này mang lại hiệu năng gần như cục bộ, độ sẵn sàng cao, và giảm 28% TCO so với on-premises.\nChiến lược triển khai này cho thấy cách tích hợp lưu trữ đám mây hiện đại có thể khắc phục các hạn chế truyền thống về băng thông, tính nhất quán và độ phức tạp vận hành trong các hệ thống phân tán nhiều địa điểm.\nTác Giả Sachin Bawse\nSachin là Kiến trúc sư Giải pháp Chuyên gia về Lưu trữ (GTM Specialist Storage Solution Architect) tại Amazon Web Services (AWS), nơi anh chuyên về tối ưu hóa các giải pháp lưu trữ, hỗ trợ di chuyển dữ liệu, và nâng cao hiệu suất khối lượng công việc cho khách hàng. Ngoài công việc, Sachin là một người đam mê khám phá, thích du lịch đến những điểm đến mới, tìm hiểu các nền văn hóa khác nhau và thưởng thức ẩm thực đa dạng. Vishnu Vashist\nVishnu Vashist là Kiến trúc sư Giải pháp Thành công Đối tác (Partner Success Solutions Architect) trong bộ phận Đối tác (Partner Org) của AWS, nơi anh phụ trách các tài khoản thuộc lĩnh vực Y tế, Chăm sóc sức khỏe, Khoa học đời sống (HCLS) cũng như Du lịch, Vận tải và Logistics. Anh chuyên về di chuyển và hiện đại hóa hệ thống, hỗ trợ các dự án di chuyển quy mô lớn lên AWS và hướng dẫn khách hàng cùng đối tác về thiết kế kiến trúc và hạ tầng trên AWS. "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Bắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Bắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "Bắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "Bắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "Bắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.4-agent-core-run/5.4.1-run-agentcore/",
	"title": "Cấu hình &amp; Deploy AgentCore",
	"tags": [],
	"description": "",
	"content": "Bắt đầu với AgentCore Configure Đầu tiên, bạn cần đẩy code từ máy local lên AWS AgentCore bằng lệnh:\nagentcore configure -e ./{ten_file_python.py} 1. Agent Name Nhập tên cho Agent của bạn.\n2. Configuration File Nhấn Enter để dùng file cấu hình mặc định pyproject.toml.\n3. Deployment Configuration Chọn 2 – Deploy bằng Docker để AgentCore tự tạo Docker image và quản lý dễ dàng hơn.\n4. Execution Role Để mặc định và cho AWS tự tạo IAM Role.\n5. ECR Repository Nhấn Enter để AWS tự tạo nơi lưu Docker image.\n6. Authorization Configuration Chọn No cho OAuth. Agent chỉ cho phép truy cập qua AWS IAM Access Key \u0026amp; Secret Key.\n7. Request Header Allowlist Nhấn Enter để dùng cấu hình mặc định.\nKết quả Hoàn tất bước này là bạn đã đẩy code lên AgentCore thành công.\nKhởi chạy Agent\nDùng lệnh sau để chạy Agent với API Key (dùng GROQ):\nagentcore launch \u0026ndash;env GROQ_API_KEY=your_api_key_here\nKhi terminal báo trạng thái Running, Agent của bạn đã hoạt động thành công.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Bài thu hoạch “Cloud Day” Mục Đích Của Sự Kiện Giới thiệu xu hướng phát triển của AI tại Việt Nam và cơ hội kinh tế. Trình bày sự tiến hóa từ Generative AI đến Agentic AI. Giới thiệu các giải pháp của AWS như Amazon Bedrock, AgentCore, và SageMaker Unified Studio trong việc xây dựng, triển khai và vận hành AI agent. Danh Sách Diễn Giả \u0026lt;Điền tên diễn giả tại đây\u0026gt; \u0026lt;Điền thêm nếu có\u0026gt; Nội Dung Nổi Bật Tác động của AI đến kinh tế Việt Nam AI có thể đóng góp 120–130 tỷ USD vào GDP Việt Nam vào năm 2040 (~25%). Thị trường AI trị giá 750 triệu USD, tăng trưởng 15–18%/năm. Việt Nam hiện có 765 startup AI, đứng thứ 2 ASEAN. Cơ hội lớn nhưng vẫn ở giai đoạn đầu, cần thêm hạ tầng, nhân tài và chính sách. Sự tiến hóa của AI → Agentic AI Generative AI Assistants → Generative AI Agents → Agentic AI Systems. Các hệ thống AI ngày càng ít phụ thuộc con người: Multi-agent systems: các agent phối hợp cùng nhau giải quyết tác vụ phức tạp. Mức độ tự động hóa tăng dần, giảm dần sự giám sát của con người: Ứng dụng của Agentic AI trong tổ chức Nâng cao năng suất làm việc, tự động hóa quy trình, thúc đẩy đổi mới và nghiên cứu. Dự đoán đến 2028, 33% ứng dụng doanh nghiệp sẽ tích hợp Agentic AI. 15% quyết định thường ngày trong doanh nghiệp sẽ được đưa ra tự động nhờ Agentic AI. Amazon Bedrock – Nền tảng AI toàn diện Cung cấp đa dạng mô hình từ nhiều hãng hàng đầu. Cho phép tùy chỉnh mô hình với dữ liệu riêng, đảm bảo bảo mật và kiểm soát chi phí. Tích hợp Responsible AI checks để đảm bảo an toàn. Hỗ trợ triển khai và vận hành agent nhanh chóng, an toàn và mở rộng dễ dàng. Amazon Bedrock AgentCore Môi trường triển khai và vận hành Agent bảo mật, có khả năng mở rộng cao. Hỗ trợ các framework: LangChain, CrewAI, LangGraph, Strands Agents. Quản lý short-term và long-term memory, có truy xuất ngữ nghĩa (semantic search). Tích hợp và khám phá tool dễ dàng. Hạ tầng dữ liệu và AI Giới thiệu Amazon SageMaker Unified Studio – trung tâm hợp nhất dữ liệu, phân tích và AI. Kết nối chặt chẽ với: Amazon Redshift, Athena, EMR, Glue – xử lý và lưu trữ dữ liệu. Amazon QuickSight – trực quan hóa dữ liệu. Amazon Bedrock – phát triển ứng dụng GenAI. Hỗ trợ tích hợp Zero-ETL giữa S3 data lake và Redshift data warehouse. Data Lakehouse Hỗ trợ nhiều loại lưu trữ: S3 Tables, Redshift Managed Storage. Kết nối các nguồn dữ liệu lớn: Aurora, DynamoDB, MSK, Kinesis, OpenSearch, Salesforce, SAP, Facebook Ads. Những Gì Học Được Về Tư Duy AI và Cloud Hiểu được xu hướng Agentic AI là giai đoạn tiếp theo của Generative AI và đang hot hiện tại. Agentic AI không chỉ là Chatbot mà là hệ thống có thể hành động và tự ra quyết định. Nắm bắt cách AWS Bedrock cung cấp nền tảng cho doanh nghiệp xây dựng hệ thống AI thông minh. Thấy rõ tầm quan trọng của AI agents trong tự động hóa doanh nghiệp và sáng tạo. Về Kiến Trúc Kỹ Thuật Hiểu mối liên kết giữa Bedrock – SageMaker – Redshift – S3 trong một hệ sinh thái AI hoàn chỉnh. Nắm được cách AWS xử lý memory, tool discovery, và observability cho AI agent. Ứng Dụng Vào Công Việc Áp dụng Amazon Berock cho dự án hiện tại: Sử dụng Amazon Titan Embeddings để tạo embedding Thử nghiệm Zero-ETL với Amazon Redshift để đơn giản hóa pipeline dữ liệu từ Aurora/DynamoDB. Đánh giá việc sử dụng Amazon Bedrock AgentCore để xây dựng các tác tử AI tự động hóa quy trình nghiệp vụ (thay vì chỉ dùng Lambda + Bedrock cơ bản). Trải nghiệm trong Sự Kiện Tham gia sự kiện Cloud Day là một trải nghiệm rất bổ ích, giúp tôi có cái nhìn rõ ràng hơn về cách các doanh nghiệp đang ứng dụng AI để hiện đại hóa hệ thống và nâng cao năng suất.\nHọc hỏi từ các diễn giả có chuyên môn cao Các diễn giả đến từ AWS chia sẻ sâu về Agentic AI và sự khác biệt so với Generative AI truyền thống. Qua các ví dụ thực tế của Amazon, tôi hiểu rõ hơn cách họ triển khai multi-agent systems để tối ưu quy trình doanh nghiệp. Trải nghiệm kỹ thuật thực tế Tìm hiểu cách hoạt động của Amazon Bedrock AgentCore, từ cách nó xử lý short/long-term memory đến quản lý công cụ tích hợp. Thấy rõ quy trình kết nối dữ liệu từ S3 – Redshift – SageMaker, và cách AI agents truy xuất dữ liệu để trả lời theo ngữ cảnh. Hiểu rõ mô hình Lakehouse và cơ chế Zero-ETL integration giữa data lake và data warehouse. Ứng dụng công cụ hiện đại Học cách triển khai Agentic AI nhanh chóng trên AWS Bedrock, với độ bảo mật và khả năng mở rộng cao. Bài học rút ra Agentic AI không chỉ là công nghệ mới, mà là bước tiến chiến lược để doanh nghiệp đạt tự động hóa cấp hệ thống. Hạ tầng AI hiện đại cần được thiết kế dựa trên dữ liệu và cloud-native architecture. AWS đang dẫn đầu trong việc cung cấp nền tảng toàn diện cho AI/ML, đặc biệt là Bedrock và SageMaker. Thấy rõ tầm quan trọng của AI agents trong tự động hóa doanh nghiệp và sáng tạo. Một Số Hình Ảnh Khi Tham Gia Sự Kiện Tổng thể, sự kiện không chỉ cung cấp kiến thức kỹ thuật mà còn giúp tôi thay đổi cách tư duy về thiết kế ứng dụng, hiện đại hóa hệ thống và phối hợp hiệu quả hơn giữa các team.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Bài thu hoạch “Generative AI with Amazon Bedrock” Mục Đích Của Sự Kiện Cung cấp kiến thức nền tảng về Generative AI và sự khác biệt so với Machine Learning truyền thống. Mô tả chi tiết về dịch vụ Amazon Bedrock và các mô hình nền tảng (Foundation Models). Hướng dẫn kỹ thuật về RAG (Retrieval Augmented Generation) để xây dựng ứng dụng AI thông minh, chính xác và tránh ảo giác. Giới thiệu hệ sinh thái các dịch vụ AI chuyên biệt của AWS. Danh Sách Diễn Giả Lam Tuan Kiet - Sr DevOps Engineer, FPT Software. Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud. Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey. Nội Dung Nổi Bật Sự chuyển dịch: Traditional ML vs Foudation Models Traditional ML Models: Chuyên biệt cho từng tác vụ cụ thể (Specific tasks), cần dữ liệu được gán nhãn (Labeled data), quy trình Train/Deploy phức tạp cho từng mục đích. Foundation Models (FM): Được huấn luyện trên dữ liệu phi cấu trúc khổng lồ (Unlabeled data), có khả năng thích ứng cho nhiều tác vụ khác nhau như: Text generation, Summarization, Q\u0026amp;A, Chatbot. Hệ sinh thái AI trên AWS Amazon Bedrock: Nơi hội tụ các mô hình FM hàng đầu từ các đối tác của AWS(AI21 Labs, Anthropic, Cohere, Meta, Stability AI,\u0026hellip;) và mô hình của Amazon. AWS Specialized AI Services: Các dịch vụ AI của AWS có thể gọi là \u0026ldquo;mì ăn liền\u0026rdquo; được tối ưu cho tác vụ cụ thể mà không cần train mô hình: Amazon Rekognition: Phát hiện đối tượng, Nhận diện khuôn mặt, Nhận diện cảm xúc, Nhận diện cảm xúc, Nhận diện người nổi tiếng, phân tích video - 0.001$/ảnh với 1 triệu ảnh đầu tiên Amazon translate: Dịch thuật văn bản đa ngôn ngữ theo thời gian thực với độ chính xác cao và văn phong tự nhiên. Amazon Textract: Trích xuất thông tin có cấu trúc (bảng biểu, form mẫu) từ văn bản quét hoặc PDF. Amazon Transcribe: Chuyển đổi giọng nói thành văn bản Amazon Polly: Chuyển đổi văn bản thành giọng nói. Amazon Comprehend: Phân tích cảm xúc văn bản, trích xuất từ khóa và phân loại chủ đề tự động. Amazon Kendra: Cho phép hỏi đáp bằng ngôn ngữ tự nhiên để tìm thông tin trong tài liệu nội bộ của doanh nghiệp. Amazon Lookout: Phát hiện các bất thường trong dây chuyền sản xuất hoặc máy móc công nghiệp để bảo trì dự đoán. Amazon Personalize: Xây dựng hệ thống gợi ý theo thời gian thực, sử dụng công nghệ máy học. Kỹ thuật Prompting: Chain of Thought (CoT) So sánh giữa Standard Prompting là hỏi thẳng kết quả và Chain-of-Thought Prompting. CoT hướng dẫn mô hình suy luận từng bước để giải quyết các bài toán logic phức tạp, giúp tăng độ chính xác đáng kể so với việc chỉ đưa ra đáp án cuối cùng. RAG (Retrieval Augmented Generation) – Trọng tâm kỹ thuật Vấn đề: Giải quyết hiện tượng \u0026ldquo;ảo giác\u0026rdquo; và thiếu kiến thức cập nhật của LLM. Giải pháp: Kết hợp khả năng truy xuất dữ liệu (Retrieval) từ Knowledge Base bên ngoài với khả năng tạo sinh (Generation) của LLM. Quy trình Data Ingestion (Nạp dữ liệu): Dữ liệu thô (New data) $\\rightarrow$ Chia nhỏ (Chunking). Đi qua Embeddings model (ví dụ: Amazon Titan Text Embeddings V2.0). Lưu trữ dưới dạng vector vào Vector Store (OpenSearch Serverless, Pinecone, Redis\u0026hellip;). RetrieveAndGenerate API: API quản lý toàn bộ quy trình từ nhận input người dùng $\\rightarrow$ tạo query embedding $\\rightarrow$ truy xuất dữ liệu $\\rightarrow$ bổ sung ngữ cảnh (augment prompt) $\\rightarrow$ sinh câu trả lời. Những Gì Học Được Về Tư Duy AI và Cloud Hiểu rõ khi nào nên dùng Specialized AI Services cho bài toán nhanh, cụ thể và khi nào dùng Bedrock/GenAI cho bài toán sáng tạo, phức tạp. Nắm vững tư duy thiết kế hệ thống RAG: Không chỉ là gọi API của LLM, mà là bài toán quản lý dữ liệu và vector hóa để cung cấp ngữ cảnh đúng cho AI tạo phản hồi chuẩn hơn. Về Kiến Trúc Kỹ Thuật Kỹ thuật Chain of Thought là chìa khóa để tối ưu hóa kết quả đầu ra của mô hình mà không cần fine-tuning. Hiểu sâu về vai trò của Amazon Titan Embeddings V2.0 trong việc chuyển đổi văn bản đa ngôn ngữ thành vector (hỗ trợ 100+ ngôn ngữ, vector size linh hoạt 256/512/1024). Ứng Dụng Vào Công Việc Áp dụng Amazon Berock cho dự án hiện tại: Amazon Rekognition: nhận diện món ăn từ ảnh để tự động điền thông tin calo, Amazon Comprehend: phân tích text để chuẩn hóa món ăn lấy và ghi dữ liệu calo. Áp dụng thử kĩ thuật RAG cho dự án hiện tại. Sử dụng Bedrock Agents để điều phối các tác vụ như truy vấn món ăn từ vector store, tính toán mục tiêu calo và xây dựng thực đơn từng ngày. Trải nghiệm trong Sự Kiện Tham gia buổi workshop Generative AI with Amazon Bedrock mang lại cái nhìn rất thực tế về cách xây dựng ứng dụng AI hiện đại, đi từ lý thuyết nền tảng đến triển khai thực tế.\nKiến thức thực chiến từ chuyên gia Các diễn giả đã giải thích khá rõ ràng luồng đi của dữ liệu trong một hệ thống RAG, giúp tôi hình dung được \u0026ldquo;hộp đen\u0026rdquo; phía sau các ứng dụng Chatbot hiện nay đang sử dụng. Việc phân tích rõ ràng giữa Traditional ML và Generative AI giúp tôi định hình lại chiến lược chọn công nghệ cho các dự án sắp tới. Trải nghiệm công nghệ Ấn tượng với RetrieveAndGenerate API của Bedrock vì nó giúp giảm bớt rất nhiều công sức code thủ công cho phần kết nối giữa Vector Store và LLM. Thấy được sức mạnh của Amazon Titan Embedding trong việc hỗ trợ đa ngôn ngữ, rất phù hợp cho các ứng dụng tại thị trường Việt Nam. Bài học rút ra RAG là tiêu chuẩn mới: Để AI ứng dụng được trong doanh nghiệp, RAG là bắt buộc để đảm bảo tính chính xác và bảo mật dữ liệu. Hệ sinh thái toàn diện: AWS cung cấp đầy đủ từ tầng hạ tầng (Vector Store) đến tầng mô hình (Bedrock) và tầng ứng dụng (Agents), giúp việc triển khai nhanh chóng hơn rất nhiều. Một Số Hình Ảnh Khi Tham Gia Sự Kiện Tổng thể, sự kiện không chỉ cung cấp kiến thức kỹ thuật mà còn giúp tôi thay đổi cách tư duy về thiết kế ứng dụng, hiện đại hóa hệ thống và phối hợp hiệu quả hơn giữa các team.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.1-workshop-overview/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Giới thiệu về WorkShop Workshop này hướng dẫn từng bước thiết lập IAM, AWS CLI, UV, Groq API, triển khai mã nguồn RAG tích hợp Groq LLM vào AWS AgentCore và cuối cùng là publish API qua AWS Gateway. Mục tiêu WorkShop \u0026ldquo;Cách gọi api\u0026rdquo; hiểu cách gọi api bên ngoài Aws Agent Core \u0026ldquo;Chunking\u0026rdquo; cách chunking chia dữ liệu cho Rag có thể lấy ra được một cách tối ưu \u0026ldquo;Thêm bộ nhớ cho Rag\u0026rdquo; tìm hiểu cách mà Agent Rag có thể nhớ được từng dữ liệu khi tương tác với người đùng \u0026ldquo;Deploy Aws Agent Core\u0026rdquo; tim hiểu cách mà triển khai được Aws Agent Core \u0026ldquo;Triển khai API\u0026rdquo; cách gọi Agent Core thông qua API ![overview](ảnh kiến trúc mô hìn) "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.4-agent-core-run/5.4.2-call-agentcore/",
	"title": "Gọi AgentCore",
	"tags": [],
	"description": "",
	"content": "Demo đơn giản với AgentCore 1. Gửi câu hỏi đầu tiên Dùng lệnh:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Tell me about roaming activations\u0026#39;}\u0026#34; Agent sẽ trả lời dựa trên dữ liệu bạn đã triển khai (database + logic trong code).\n2. Kiểm tra khả năng ghi nhớ giữa các lần gọi (session) Sau khi hỏi lần đầu, bạn tiếp tục hỏi một câu có liên quan \u0026mdash; ví dụ\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Activate it for Vietnam\u0026#39;}\u0026#34; agentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;which country was i referring to\u0026#39;}\u0026#34; Nếu Agent phản hồi đúng ý và nhớ thông tin trước đó → chứng tỏ Memory hoạt động và AgentCore đang lưu ngữ cảnh giữa các lần gọi.\nAgentCore hoạt động đúng như mong đợi trong demo này.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/",
	"title": "Nhật ký công việc",
	"tags": [],
	"description": "",
	"content": "Trong trang này, tôi tổng hợp nhật ký công việc thực tập tại First Cloud Journey (FCJ). Chương trình thực tập kéo dài 12 tuần, trong đó tôi lần lượt hoàn thành các mục tiêu và nhiệm vụ hàng tuần để nắm vững các dịch vụ AWS cơ bản, triển khai dự án thực hành và làm quen với hệ sinh thái Cloud của AWS.\nTrong suốt 12 tuần, tôi đã thực hiện các công việc chính như sau:\nTuần 1: Làm quen với AWS và các dịch vụ cơ bản trong AWS\nTuần 2: Tìm hiểu Networking và các dịch vụ liên quan\nTuần 3: Triển khai và quản lý EC2, EBS, Security Group\nTuần 4: Làm quen với cơ sở dữ liệu RDS và NoSQL DynamoDB\nTuần 5: Sử dụng Python SDK (Boto3) để quản lý tài nguyên AWS\nTuần 6: Tìm hiểu Amazon ElastiCache và tối ưu hóa truy xuất dữ liệu\nTuần 7: Triển khai kiến trúc Serverless với Lambda, API Gateway, SQS và SNS\nTuần 8: Tự động hóa tạo tài nguyên với CloudFormation, CDK và quản lý hệ thống với SSM\nTuần 9: Khám phá hệ sinh thái dữ liệu và Machine Learning trên AWS: Glue, Athena, QuickSight, SageMaker\nTuần 10: Giám sát hệ thống với CloudWatch, CloudTrail và sử dụng AWS Amplify\nTuần 11: Hiểu quy trình CI/CD với AWS Code Series và trải nghiệm dịch vụ AI như Polly, Rekognition\nTuần 12: Ôn tập Serverless \u0026amp; AI, thiết kế DynamoDB, tích hợp API, bảo mật và giám sát hệ thống\nNhận xét chung Qua 12 tuần thực tập, tôi đã:\nTích lũy kiến thức nền tảng vững chắc về AWS, từ các dịch vụ Compute, Storage, Networking đến Database và các dịch vụ hỗ trợ. Biết triển khai và quản lý tài nguyên cơ bản, kết hợp Console và CLI một cách linh hoạt. Thực hành các dự án từ Serverless, CloudFormation, CDK, EC2, RDS, DynamoDB đến các dịch vụ AI và Machine Learning. Làm quen với môi trường thực tế tại FCJ, tương tác và phối hợp với các thành viên, nắm vững quy định, nội quy và quy trình thực tập. Những trải nghiệm này đã tạo tiền đề vững chắc cho việc triển khai các dự án cloud phức tạp hơn trong tương lai, đồng thời nâng cao kỹ năng quản lý, giám sát và tối ưu hóa hạ tầng AWS.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.1-week1/",
	"title": "Worklog Tuần 1",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 1: Kết nối, làm quen với các thành viên trong First Cloud Journey. Hiểu dịch vụ AWS cơ bản, cách tạo và quản lý chi phi với tài khoản AWS. Cách dùng console \u0026amp; CLI để tương tác và quản lý các dịch vụ. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Làm quen với các thành viên FCJ - Đọc và lưu ý các nội quy, quy định tại đơn vị thực tập 08/09/2025 08/09/2025 3 - Tìm hiểu AWS và các loại dịch vụ cơ bản + Compute (EC2) + Storage (S3) + Networking (VPC) + Database (RDS) 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tạo AWS Free Tier account - Tìm hiểu AWS Console \u0026amp; AWS CLI - Thực hành: + Tạo AWS account + Quản lý danh tính và quyền truy cập + Cài AWS CLI \u0026amp; cấu hình + Sử dụng AWS CLI với các thao tác cơ bản 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tìm hiểu cách quản lý chi phí hiệu quả với AWS budget + Budget + Cost Budget, + Usage Budget + Reservation (RI) Budget + Saving plans Budget - Thực hành: + Tạo Cost Budget + Tạo Usage Budget + Tạo RI Budget + Tạo Savings Plans Budget + Dọn Dẹp Tài Nguyên 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Tìm hiểu về dịch vụ AWS Support - Các gói hỗ trợ của AWS + Gói Basic, Developer, Business và Enterprise - Các loại yêu cầu hỗ trợ + Hỗ trợ Tài khoản và Thanh toán + Hỗ trợ nâng hạn mức dịch vụ + Hỗ trợ Kỹ thuật - Thực hành: + Chọn gói hỗ trợ Basic + Tạo yêu cầu hỗ trợ 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 1: Hiểu AWS là gì và nắm được các nhóm dịch vụ cơ bản:\nCompute: Cung cấp tài nguyên xử lý cho ứng dụng như máy ảo, container,\u0026hellip; Storage: Dùng để lưu trữ dữ liệu, sao lưu và phục hồi Networking: Quản lý hạ tầng mạng, bảo mật, và kết nối giữa các tài nguyên AWS. Database: Cung cấp dịch vụ quản lý cơ sở dữ liệu quan hệ và phi quan hệ. Đã tạo cấu hình và định danh AWS Free Tier account thành công.\nĐã biết tạo và quản lý Group user, User.\nBiết cách đăng nhập bằng IAM và các user trong cùng một group sẽ được dùng chung quyền được cấp.\nLàm quen với AWS Management Console và biết cách tìm, truy cập, sử dụng dịch vụ từ giao diện web.\nCài đặt và cấu hình AWS CLI trên máy tính bao gồm:\nAccess Key Secret Key Region mặc định Sử dụng AWS CLI để thực hiện các thao tác cơ bản như:\nKiểm tra thông tin tài khoản \u0026amp; cấu hình Lấy danh sách region Tạo và xóa S3 Bucket Sử dụng SNS amazon Tạo IAM group, user và thêm user Tạo và xóa acess key Tạo và cấu hình cơ bản VPS Chạy và chấm dứt EC2 Nắm được cách quản lý và giám sát chi phí trên AWS thông qua các công cụ:\nTạo và cấu hình các gói Budget (Cost, Usage, RI, Savings Plan). Biết cách dọn dẹp tài nguyên để quản lý chi phí hiệu quả. Hiểu về các gói hỗ trợ của AWS và biết cách tạo yêu cầu hỗ trợ từ trung tâm hỗ trợ.\nBasic: Miễn phí, hỗ trợ các vấn đề liên quan đến tài khoản và thanh toán từ trung tâm trợ giúp Developer: 29 USD/tháng, tư vấn kiến trúc cơ bản, và hỗ trợ kỹ thuật không giới hạn được tạo từ tài khoản gốc (root user) Business:100 USD/tháng, lựa chọn phổ biến cho các doanh nghiệp vừa và nhỏ với các hỗ trợ như: Chỉ dẫn theo Use-case cụ thể, Hỗ trợ sử dụng AWS Support API, không giới hạn các yêu cầu hỗ trợ được tạo bởi tất cả các IAM User,\u0026hellip; Enterprise: 15.000 USD/tháng, cho doanh nghiệp quy mô lớn được đảm bảo các tiêu chí bảo mẩ tiêu chuẩn và nghiêm ngặt nhất với các dịch vụ bảo mật như: về kiến trúc phần mềm, hạ tầng, hỗ trợ toàn diện về chiến lược và tối ưu chi phí, được ưu tiên chăm sóc đặc biệt các yêu cấu hỗ trợ,\u0026hellip; Làm quen với giao diện AWS Console và sử dụng tốt các thao tác cơ bản qua cả Console và CLI.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.2-week2/",
	"title": "Worklog Tuần 2",
	"tags": [],
	"description": "",
	"content": "Định hướng học tập và thực hành trong tuần 2 Tìm hiểu cách xây dựng hạ tầng mạng trên AWS theo mô hình chuẩn. Nắm được các cơ chế bảo mật trong môi trường VPC. Thiết lập các hình thức kết nối an toàn giữa hệ thống nội bộ và hệ thống Cloud. Nội dung triển khai trong tuần Thứ Hoạt động chính Bắt đầu Kết thúc Tham khảo 2 Nghiên cứu tổng quan Amazon VPC, phạm vi hoạt động theo Region, AZ, CIDR; tìm hiểu các thành phần mạng như Subnet, Route Table, Internet Gateway, NAT Gateway; tìm hiểu cơ chế bảo mật bằng Security Group và Network ACL; thực hành tạo mới VPC và các thành phần cơ bản 11/08/2025 11/08/2025 3 Tìm hiểu EC2 cơ bản: loại instance, AMI, key pair, cấu hình mạng; các hình thức kết nối vào EC2; tìm hiểu Elastic IP, Reachability Analyzer, Session Manager và CloudWatch 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Triển khai hệ thống EC2 đa vùng khả dụng; tạo NAT Gateway; giám sát VPC bằng CloudWatch; thiết lập Instance Connect Endpoint, sử dụng Session Manager; thực hiện dọn dẹp toàn bộ tài nguyên sau khi hoàn thành 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Tìm hiểu các mô hình kết nối mạng: Site-to-Site VPN, Transit Gateway và VPC Peering 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Thực hành cấu hình VPN kết nối giữa AWS và môi trường giả lập on-premise; thiết lập VPC Peering và Transit Gateway 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Tổng hợp kết quả đạt được trong tuần 2 Trong tuần thứ hai, em đã tập trung tìm hiểu và xây dựng hệ thống mạng cơ bản trên AWS, đồng thời triển khai các mô hình kết nối và bảo mật phổ biến trong thực tế.\nCụ thể, em đã tự tay thiết kế và tạo thành công một hệ thống VPC hoàn chỉnh, bao gồm một VPC chính với dải địa chỉ IP 10.0.0.0/16. Bên trong VPC, em triển khai hai subnet public và private nằm ở hai Availability Zone khác nhau để đảm bảo khả năng sẵn sàng cao. Hệ thống định tuyến được cấu hình đầy đủ thông qua Route Table, Internet Gateway và NAT Gateway nhằm đảm bảo các máy chủ ở private subnet vẫn có thể truy cập Internet một cách an toàn. Đồng thời, em cũng kích hoạt VPC Flow Logs và theo dõi lưu lượng mạng thông qua CloudWatch Logs.\nBên cạnh đó, em đã nắm được quy trình triển khai và vận hành EC2, bao gồm việc tạo instance tại cả public subnet và private subnet, gán Elastic IP cho máy chủ public và thực hiện các hình thức truy cập như SSH, EC2 Instance Connect và Session Manager. Việc giám sát tài nguyên EC2 được thực hiện thông qua Amazon CloudWatch với các chỉ số về CPU, Network và trạng thái hệ thống.\nVề mặt bảo mật, em đã cấu hình thành công Security Group để kiểm soát truy cập ở mức máy chủ và Network ACL để kiểm soát ở mức subnet. Ngoài ra, em cũng thiết lập CloudWatch Alarm để cảnh báo khi tài nguyên EC2 có dấu hiệu hoạt động bất thường.\nTrong phần kết nối mạng nâng cao, em đã tìm hiểu và thực hành mô hình AWS Site-to-Site VPN, bao gồm việc tạo Virtual Private Gateway, Customer Gateway và thiết lập đường hầm IPSec giữa AWS và hệ thống on-premise mô phỏng. Đồng thời, em cũng triển khai VPC Peering nhằm kết nối hai VPC trong cùng khu vực, và làm quen với mô hình AWS Transit Gateway để kết nối nhiều mạng với nhau theo kiến trúc tập trung.\nNgoài ra, em cũng hiểu rõ hơn cách sử dụng AWS Systems Manager trong quản lý và vận hành hệ thống mà không cần truy cập trực tiếp thông qua SSH, giúp tăng mức độ bảo mật cho hạ tầng.\nThông qua các nội dung đã thực hiện, em đã nắm được nền tảng về thiết kế hạ tầng mạng, bảo mật và kết nối trên AWS, tạo tiền đề cho việc triển khai các hệ thống phức tạp hơn trong các tuần tiếp theo.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.3-week3/",
	"title": "Worklog Tuần 3",
	"tags": [],
	"description": "",
	"content": "Định hướng học tập trong tuần 3 Tìm hiểu sâu về dịch vụ máy chủ ảo Amazon EC2 – nền tảng cốt lõi của hạ tầng AWS. Nắm được cách vận hành, giám sát và mở rộng hệ thống máy chủ theo mô hình thực tế. Nội dung triển khai trong tuần Thứ Hoạt động chính Bắt đầu Kết thúc Tham khảo 2 Nghiên cứu tổng quan về dịch vụ EC2 và các thành phần liên quan như: instance type, AMI, key pair, EBS, instance store, user data, meta data, auto scaling, EFS/FSx, Lightsail và MGN 11/08/2025 11/08/2025 3 Thực hành tạo EC2 trên Windows và Linux, snapshot sao lưu dữ liệu, cài đặt ứng dụng trên EC2, quản lý tài nguyên bằng tag và resource group, phân quyền với IAM và dọn dẹp tài nguyên 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Tìm hiểu và cấu hình Amazon CloudWatch: giám sát chỉ số, thu thập log, thiết lập cảnh báo và xây dựng dashboard theo dõi trạng thái hệ thống 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Nghiên cứu mô hình EC2 Auto Scaling, các cơ chế scale, launch template và elastic load balancer; thực hành triển khai hệ thống có khả năng tự động mở rộng và thu hẹp 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Làm quen với Amazon Lightsail, triển khai các instance ứng dụng và database, cấu hình load balancer, snapshot backup và nâng cấp cấu hình máy chủ 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Tổng hợp kết quả đạt được trong tuần 3 Trong tuần thứ ba, em tập trung tìm hiểu chuyên sâu về dịch vụ EC2 – thành phần quan trọng trong việc triển khai hạ tầng máy chủ trên AWS. Thông qua quá trình học tập và thực hành, em đã hiểu rõ hơn về kiến trúc của EC2 cũng như cách lựa chọn cấu hình phù hợp với từng nhu cầu sử dụng. Em có thể phân biệt được các nhóm instance phổ biến như nhóm tổng hợp, nhóm tối ưu CPU, tối ưu bộ nhớ và tối ưu lưu trữ.\nBên cạnh đó, em đã nắm được vai trò của AMI, key pair, EBS và instance store trong quá trình khởi tạo và vận hành máy chủ. Việc sử dụng user data và meta data cũng giúp em tự động hóa quá trình thiết lập môi trường ngay từ khi EC2 được khởi tạo. Ngoài ra, em cũng làm quen với EFS và FSx để phục vụ nhu cầu lưu trữ và chia sẻ dữ liệu giữa nhiều máy chủ.\nVề thực hành, em đã tự triển khai và kết nối thành công máy chủ EC2 trên cả hai hệ điều hành Windows và Linux. Em biết cách tạo snapshot để sao lưu dữ liệu và khôi phục khi cần thiết, đồng thời cài đặt được các ứng dụng web cơ bản phục vụ cho việc thử nghiệm. Việc sử dụng tag và resource group giúp em quản lý tài nguyên một cách khoa học hơn. Đồng thời, thông qua IAM, em đã biết cách giới hạn quyền truy cập nhằm đảm bảo an toàn cho hệ thống.\nTrong phần giám sát, em đã tìm hiểu và áp dụng Amazon CloudWatch để theo dõi hoạt động của hệ thống. Em cấu hình được các chỉ số giám sát EC2, thu thập log từ ứng dụng, tạo cảnh báo khi CPU hoặc lưu lượng mạng vượt ngưỡng và xây dựng dashboard tổng hợp giúp theo dõi tình trạng toàn bộ hệ thống một cách trực quan.\nVề khả năng mở rộng, em đã hiểu rõ cơ chế hoạt động của EC2 Auto Scaling, bao gồm cách mở rộng thủ công, mở rộng theo lịch và mở rộng tự động dựa trên tải thực tế. Em cũng đã triển khai thành công auto scaling group kết hợp với load balancer để đảm bảo hệ thống có khả năng tự điều chỉnh số lượng máy chủ khi lưu lượng thay đổi.\nCuối tuần, em tiếp cận thêm dịch vụ Amazon Lightsail – một nền tảng triển khai nhanh các ứng dụng phổ biến. Em đã thực hành triển khai các mô hình như website WordPress, hệ thống bán hàng PrestaShop và phần mềm kế toán Akaunting. Ngoài ra, em cũng sử dụng Lightsail Database để lưu trữ dữ liệu, Lightsail Load Balancer để phân phối tải, tạo snapshot để sao lưu và thực hiện nâng cấp cấu hình instance khi cần thiết.\nThông qua các nội dung đã thực hiện trong tuần 3, em đã nắm được quy trình triển khai, vận hành, giám sát và mở rộng hệ thống máy chủ trên AWS, tạo nền tảng vững chắc để tiếp tục học và triển khai các hệ thống thực tế ở những giai đoạn tiếp theo.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.4-week4/",
	"title": "Worklog Tuần 4",
	"tags": [],
	"description": "",
	"content": "Định hướng học tập trong tuần 4 Tìm hiểu các dịch vụ lưu trữ dữ liệu trên AWS, đặc biệt là S3 và các giải pháp lưu trữ lai (hybrid). Nắm được các cơ chế sao lưu, khôi phục và chiến lược phòng chống thảm họa cho hệ thống. Thực hành triển khai các mô hình lưu trữ có tính sẵn sàng cao, an toàn và dễ mở rộng. Nội dung triển khai trong tuần Thứ Hoạt động chính Bắt đầu Kết thúc Tham khảo 2 Tìm hiểu chi tiết về Amazon S3: bucket, object, access point, các lớp lưu trữ, versioning, CORS, control access, static website, CloudFront, replication và tối ưu hiệu năng; thực hành tạo bucket, upload dữ liệu, triển khai static website và sao chép dữ liệu giữa các region 11/08/2025 11/08/2025 3 Tìm hiểu kiến trúc AWS Backup, xây dựng kế hoạch sao lưu, thiết lập thông báo qua SNS; thực hành tạo Backup Plan, theo dõi hoạt động backup và kiểm thử khả năng khôi phục 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Tìm hiểu Snow Family, Storage Gateway (File, Volume, Tape Gateway) và các khái niệm Disaster Recovery (RTO, RPO); thực hành tạo File Gateway, chia sẻ dữ liệu và kết nối với môi trường on-premise 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Tìm hiểu AWS Import/Export; thực hành import máy ảo từ VMware lên AWS và export EC2 từ AWS về môi trường on-premise 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Tìm hiểu dịch vụ Amazon FSx; thực hành triển khai hệ thống file server Multi-AZ với SSD và HDD, kiểm tra hiệu năng, giám sát, cấu hình snapshot, scale dung lượng và throughput 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Tổng hợp kết quả đạt được trong tuần 4 Trong tuần thứ tư, em tập trung tìm hiểu các giải pháp lưu trữ dữ liệu trên AWS cũng như các cơ chế sao lưu và khôi phục nhằm đảm bảo an toàn cho hệ thống.\nTrước hết, em đã nắm được cách hoạt động của Amazon S3, bao gồm cấu trúc bucket, object, access point cùng các cơ chế kiểm soát truy cập thông qua ACL, policy, CORS và versioning. Em thực hành tạo bucket, upload dữ liệu, triển khai website tĩnh trực tiếp trên S3, đồng thời tích hợp CloudFront để tăng tốc phân phối nội dung. Ngoài ra, em cũng thực hiện sao chép dữ liệu giữa các region nhằm nâng cao khả năng sẵn sàng.\nBên cạnh S3, em đã tìm hiểu các giải pháp lưu trữ kết hợp giữa on-premise và AWS thông qua Storage Gateway, bao gồm File Gateway, Volume Gateway và Tape Gateway. Việc kết nối file share từ môi trường nội bộ lên AWS giúp em hiểu rõ hơn mô hình lưu trữ hybrid trong thực tế. Em cũng tìm hiểu Snow Family và các kịch bản di chuyển dữ liệu lớn trong doanh nghiệp.\nVề sao lưu và khôi phục dữ liệu, em đã nghiên cứu kiến trúc của AWS Backup, cách xây dựng Backup Plan, cơ chế lưu trữ trong Backup Vault và quy trình khôi phục dữ liệu. Em cấu hình thành công hệ thống sao lưu tự động, tích hợp SNS để gửi thông báo và thực hiện kiểm thử restore dữ liệu. Đồng thời, em cũng hiểu rõ các chiến lược Disaster Recovery như RTO, RPO, Pilot Light, Warm Standby và Active-Active để đảm bảo khả năng vận hành liên tục cho hệ thống khi xảy ra sự cố.\nTrong phần lưu trữ file hệ thống, em đã thực hành với Amazon FSx bằng cách triển khai các mô hình Multi-AZ với ổ SSD và HDD, tạo file share, theo dõi hiệu năng, bật cơ chế chống trùng lặp dữ liệu, shadow copy và quản lý phiên làm việc của người dùng. Ngoài ra, em cũng thực hiện mở rộng dung lượng lưu trữ và thông lượng để đáp ứng nhu cầu sử dụng tăng cao.\nNgoài ra, em đã thực hành quy trình Import/Export máy ảo giữa môi trường on-premise và AWS. Cụ thể, em export máy ảo từ VMware, upload lên S3, import thành AMI và khởi chạy EC2. Ở chiều ngược lại, em cũng thực hiện export EC2 về lại môi trường ảo hóa nội bộ.\nThông qua các nội dung đã thực hiện trong tuần 4, em đã nắm được toàn cảnh về các giải pháp lưu trữ, sao lưu, khôi phục và phòng chống thảm họa trên AWS. Đây là nền tảng quan trọng để xây dựng các hệ thống có tính sẵn sàng cao, an toàn và dễ mở rộng trong thực tế.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.5-week5/",
	"title": "Worklog Tuần 5",
	"tags": [],
	"description": "",
	"content": "Định hướng học tập trong tuần 5 Tìm hiểu tổng quan về mô hình bảo mật của AWS và các nguyên tắc bảo mật cốt lõi. Nắm được cách quản lý danh tính, phân quyền và xác thực người dùng trên nền tảng AWS. Thực hành các cơ chế mã hóa, giám sát và bảo vệ dữ liệu trong môi trường cloud. Nội dung triển khai trong tuần Thứ Hoạt động chính Bắt đầu Kết thúc Tham khảo 2 Tìm hiểu mô hình Shared Responsibility Model, trách nhiệm giữa AWS và khách hàng; nghiên cứu tổng quan về Amazon IAM gồm Root Account, IAM User, IAM Group, IAM Policy, IAM Role và Permission Boundary 11/08/2025 11/08/2025 3 Thực hành tạo IAM Group, IAM User, IAM Role và Assume Role; cấu hình người dùng quản trị EC2, RDS; thiết lập điều kiện Role theo IP, thời gian; tạo policy giới hạn quyền và kiểm thử tài khoản bị giới hạn 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Tìm hiểu Tag và AWS Resource Groups; thực hành gắn Tag cho EC2 bằng Console và CLI; tạo Resource Group; kiểm soát truy cập EC2 dựa trên Tag thông qua IAM Policy và kiểm thử các trường hợp bị từ chối truy cập 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Tìm hiểu Amazon Cognito, AWS Organizations, AWS Identity Center (SSO), AWS KMS và AWS Security Hub; kích hoạt Security Hub và đánh giá các bộ tiêu chuẩn bảo mật 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Thực hành AWS SSO với mô hình multi-account; tạo OU trong AWS Organizations; mã hóa dữ liệu với AWS KMS; upload dữ liệu mã hóa lên S3; bật CloudTrail và sử dụng Athena truy vấn log; kiểm thử chia sẻ dữ liệu mã hóa 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Tổng hợp kết quả đạt được trong tuần 5 Trong tuần thứ năm, em tập trung tìm hiểu sâu về hệ thống bảo mật và quản lý danh tính trên nền tảng AWS. Trước hết, em đã nắm vững mô hình chia sẻ trách nhiệm (Shared Responsibility Model), trong đó AWS chịu trách nhiệm bảo mật hạ tầng vật lý, phần cứng và các dịch vụ nền tảng, còn khách hàng chịu trách nhiệm cấu hình bảo mật dữ liệu, ứng dụng, phân quyền truy cập và các cơ chế mã hóa. Điều này giúp em hiểu rõ ranh giới trách nhiệm khi vận hành hệ thống trên môi trường cloud.\nTiếp theo, em nghiên cứu chi tiết dịch vụ Amazon Identity and Access Management (IAM). Em đã nắm được các khái niệm cốt lõi như Root Account, IAM User, IAM Group, IAM Policy, IAM Role và Permission Boundary. Thông qua quá trình thực hành, em đã tạo nhiều nhóm người dùng, cấu hình tài khoản quản trị cho từng dịch vụ như EC2, RDS, đồng thời áp dụng nguyên tắc phân quyền tối thiểu (least privilege) để hạn chế rủi ro bảo mật. Ngoài ra, em cũng thực hiện cấu hình Assume Role, giới hạn theo IP và thời gian truy cập, từ đó kiểm thử các kịch bản truy cập bị từ chối khi không thỏa điều kiện bảo mật.\nBên cạnh IAM, em đã tìm hiểu và thực hành sử dụng Tag để quản lý tài nguyên và kiểm soát quyền truy cập. Em gắn Tag cho các tài nguyên EC2 thông qua cả Console và CLI, tạo Resource Groups để gom nhóm tài nguyên theo thẻ, sau đó thiết lập IAM Policy cho phép hoặc từ chối truy cập dựa trên giá trị Tag và Region. Việc kiểm thử các trường hợp bị deny do sai Region, sai Tag hoặc không thỏa điều kiện giúp em hiểu rõ hơn cách AWS kiểm soát truy cập ở mức tài nguyên.\nTrong phần quản lý người dùng ứng dụng, em tìm hiểu dịch vụ Amazon Cognito, bao gồm User Pool để quản lý tài khoản và xác thực đăng nhập, cùng Identity Pool để cấp quyền truy cập tạm thời vào các dịch vụ AWS. Đồng thời, em cũng nghiên cứu mô hình vận hành đa tài khoản thông qua AWS Organizations và AWS Identity Center (SSO). Em đã thực hành tạo Organizational Unit (OU), phân tách môi trường, thiết lập đăng nhập tập trung bằng SSO, từ đó hiểu rõ cách quản lý tập trung trong hệ thống multi-account.\nVề bảo mật dữ liệu, em đã học cách sử dụng AWS Key Management Service (KMS) để tạo và quản lý khóa mã hóa. Em áp dụng mã hóa cho dữ liệu lưu trữ trong Amazon S3, sau đó kiểm thử việc upload, truy cập và chia sẻ dữ liệu đã được mã hóa thông qua các quyền được cấp bởi IAM và Role. Đồng thời, em bật AWS CloudTrail để ghi lại toàn bộ hoạt động của tài khoản và sử dụng Amazon Athena để truy vấn log phục vụ công tác giám sát và kiểm tra an ninh.\nNgoài ra, em cũng kích hoạt và sử dụng AWS Security Hub để tổng hợp, đánh giá các tiêu chuẩn bảo mật mặc định. Thông qua các báo cáo và cảnh báo, em hiểu được các lỗi cấu hình phổ biến cũng như cách cải thiện mức độ an toàn của hệ thống.\nThông qua các nội dung thực hiện trong tuần 5, em đã có cái nhìn toàn diện về hệ sinh thái bảo mật của AWS, từ quản lý danh tính, phân quyền truy cập, kiểm soát tài nguyên bằng Tag, đến mã hóa dữ liệu, giám sát hoạt động và đánh giá tiêu chuẩn bảo mật. Đây là nền tảng quan trọng để triển khai các hệ thống cloud an toàn và đáp ứng yêu cầu vận hành thực tế.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.6-week6/",
	"title": "Worklog Tuần 6",
	"tags": [],
	"description": "",
	"content": "Định hướng học tập trong tuần 6 Tìm hiểu các dịch vụ cơ sở dữ liệu quan hệ (SQL) và phi quan hệ (NoSQL) trên nền tảng AWS. Biết cách triển khai, vận hành và sao lưu dữ liệu an toàn trong môi trường cloud. Làm quen với các cơ chế tăng tốc truy vấn bằng bộ nhớ đệm (cache) thông qua Amazon ElastiCache. Thực hành lập trình tương tác với cơ sở dữ liệu AWS thông qua Python SDK. Nội dung triển khai trong tuần Thứ Hoạt động chính Bắt đầu Kết thúc Tham khảo 2 Tìm hiểu tổng quan về Amazon RDS và Amazon Aurora; các hệ quản trị cơ sở dữ liệu như MySQL, PostgreSQL; kiến trúc Multi-AZ; cơ chế Read Replicas; sao lưu và khôi phục dữ liệu tự động 11/08/2025 11/08/2025 3 Thực hành triển khai hệ thống RDS: chuẩn bị hạ tầng gồm VPC, EC2, Security Group và DB Subnet Group; triển khai EC2 và RDS trong Private Subnet; kết nối EC2 với RDS; triển khai ứng dụng; cấu hình sao lưu tự động và dọn dẹp tài nguyên 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Tìm hiểu Amazon DynamoDB (NoSQL): cấu trúc bảng, Primary Key, Sort Key, mô hình Read/Write Capacity (On-demand, Provisioned); thực hành tạo bảng, ghi – đọc – cập nhật – truy vấn dữ liệu, tạo Global Secondary Index và truy vấn dữ liệu qua index 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Tìm hiểu AWS SDK cho Python gồm Botocore và Boto3; cấu hình AWS CLI; sử dụng Python thao tác với DynamoDB: tạo bảng, ghi, đọc, cập nhật, xóa dữ liệu, quét bảng, tải dữ liệu mẫu và xóa toàn bộ tài nguyên 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Tìm hiểu Amazon ElastiCache for Redis: cluster, node, shard; tạo Access Key cấu hình AWS CLI; tạo ElastiCache Cluster qua Console và CLI; sử dụng AWS SDK để đọc – ghi dữ liệu: string, hash, publish/subscribe và stream 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Tổng hợp kết quả đạt được trong tuần 6 Trong tuần thứ sáu, em tập trung tìm hiểu và thực hành về hệ thống cơ sở dữ liệu và các dịch vụ lưu trữ, xử lý dữ liệu trên nền tảng AWS. Trước tiên, em đã nghiên cứu dịch vụ Amazon RDS và Aurora, hiểu rõ đặc điểm của các engine cơ sở dữ liệu phổ biến như MySQL và PostgreSQL, cũng như cách AWS đảm bảo tính sẵn sàng cao thông qua kiến trúc Multi-AZ. Bên cạnh đó, em cũng nắm được cơ chế Read Replicas nhằm tăng hiệu năng đọc và khả năng mở rộng hệ thống, cùng với cơ chế sao lưu và khôi phục dữ liệu tự động để đảm bảo an toàn dữ liệu.\nTrong phần thực hành RDS, em đã triển khai hoàn chỉnh một hệ thống bao gồm EC2 và RDS trong môi trường Private Subnet. Em cấu hình Security Group và DB Subnet Group phù hợp để đảm bảo kết nối giữa ứng dụng và cơ sở dữ liệu diễn ra an toàn. Em cũng thực hiện backup tự động và kiểm thử khôi phục dữ liệu, qua đó hiểu rõ hơn quy trình vận hành thực tế của một hệ quản trị cơ sở dữ liệu trên cloud.\nTiếp theo, em tìm hiểu về Amazon DynamoDB – dịch vụ cơ sở dữ liệu NoSQL của AWS. Em nắm được cấu trúc bảng, vai trò của Partition Key và Sort Key, cũng như nguyên lý phân bổ dung lượng Read/Write theo hai chế độ On-demand và Provisioned. Trong quá trình thực hành, em đã tạo bảng dữ liệu, thực hiện các thao tác ghi, đọc, cập nhật, truy vấn dữ liệu, đồng thời xây dựng Global Secondary Index để tối ưu truy vấn theo các trường dữ liệu khác nhau.\nSong song với đó, em cũng làm quen với hệ sinh thái AWS SDK cho Python, bao gồm Botocore và Boto3. Em cấu hình AWS CLI để kết nối với tài khoản và sử dụng Python để lập trình thao tác trực tiếp với DynamoDB như tạo bảng, thêm dữ liệu, truy vấn, quét toàn bộ bảng và xóa dữ liệu. Việc lập trình tự động giúp em hiểu sâu hơn cách các ứng dụng thực tế làm việc với dịch vụ AWS thông qua API.\nNgoài ra, em còn tìm hiểu và thực hành với Amazon ElastiCache for Redis – dịch vụ bộ nhớ đệm phân tán của AWS. Em đã tạo ElastiCache Cluster, kết nối thông qua AWS SDK và thực hiện các thao tác đọc – ghi dữ liệu như set/get string, set/get hash, publish/subscribe và làm việc với stream. Thông qua đó, em hiểu được vai trò quan trọng của cache trong việc tăng tốc truy xuất dữ liệu và giảm tải cho hệ thống cơ sở dữ liệu chính.\nKết thúc tuần học, em đã có cái nhìn tương đối toàn diện về các mô hình cơ sở dữ liệu trên AWS, từ cơ sở dữ liệu quan hệ, NoSQL đến hệ thống cache. Đồng thời, em cũng nắm được cách triển khai, lập trình, sao lưu và tối ưu hiệu năng dữ liệu – đây là nền tảng quan trọng cho việc xây dựng và vận hành các hệ thống ứng dụng trên môi trường cloud.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.7-week7/",
	"title": "Worklog Tuần 7",
	"tags": [],
	"description": "",
	"content": "Định hướng học tập trong tuần 7 Làm quen với kiến trúc Serverless thông qua AWS Lambda và Amazon API Gateway. Hiểu nguyên lý hoạt động của các hệ thống Event-Driven trên nền tảng AWS. Biết cách xây dựng hệ thống xử lý bất đồng bộ bằng Amazon SQS và SNS. Thực hành xây dựng API kết nối Lambda với DynamoDB và S3 để xử lý dữ liệu. Nội dung triển khai trong tuần Thứ Hoạt động chính Bắt đầu Kết thúc Tham khảo 2 Tìm hiểu về AWS Lambda: khái niệm function, cách phân quyền thông qua IAM Role; tìm hiểu Amazon API Gateway: các HTTP method, cơ chế CORS; thực hành tạo IAM Role cho Lambda, xây dựng Lambda function xử lý upload file, cấu hình API Gateway trigger Lambda và lưu dữ liệu vào DynamoDB 11/08/2025 11/08/2025 3 Thực hành tích hợp Lambda với S3 và DynamoDB: tạo Lambda xử lý ảnh, tạo S3 Bucket, xây dựng IAM Policy cho Lambda, tạo và quản lý bảng trong DynamoDB, xây dựng Lambda function ghi dữ liệu và dọn dẹp tài nguyên 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Thực hành triển khai website tĩnh với S3: tạo Bucket, bật static hosting, gán policy và upload frontend; xây dựng hệ thống CRUD với DynamoDB và Lambda, cấu hình API Gateway, kiểm thử API bằng Postman và frontend, dọn dẹp tài nguyên 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Tìm hiểu kiến trúc Event-Driven với Amazon SQS và SNS: nguyên lý publish/subscribe và message queue; thực hành tạo Queue và Topic SNS, xây dựng Lambda và API để tương tác với Queue và SNS, kiểm thử hoạt động hệ thống 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Tìm hiểu AWS Step Functions và vai trò trong việc điều phối (orchestration) các microservices trong hệ thống Serverless 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Tổng hợp kết quả đạt được trong tuần 7 Trong tuần thứ bảy, em tập trung tìm hiểu và thực hành các mô hình kiến trúc Serverless và Event-Driven trên nền tảng AWS. Trước hết, em đã nghiên cứu cách hoạt động của AWS Lambda, hiểu được cơ chế triển khai hàm xử lý không cần quản lý máy chủ, cũng như vai trò của IAM Role trong việc cấp quyền truy cập tài nguyên cho Lambda theo nguyên tắc tối thiểu (least privilege).\nSong song với đó, em tìm hiểu về Amazon API Gateway, bao gồm cách xây dựng các REST API, sử dụng các phương thức HTTP và thiết lập CORS để cho phép frontend gọi API một cách an toàn. Trong quá trình thực hành, em đã xây dựng thành công hệ thống trong đó API Gateway đóng vai trò làm cổng trung gian, kích hoạt Lambda xử lý request và ghi dữ liệu xuống DynamoDB.\nTiếp theo, em thực hành tích hợp Lambda với Amazon S3 và DynamoDB. Em đã xây dựng Lambda function để xử lý upload file lên S3, đồng thời lưu metadata xuống DynamoDB. Thông qua nội dung này, em hiểu rõ hơn quy trình xây dựng một hệ thống backend serverless hoàn chỉnh với ba thành phần chính: API Gateway – Lambda – DynamoDB.\nNgoài ra, em cũng triển khai thành công mô hình host website tĩnh bằng S3, kết hợp frontend gọi API thông qua API Gateway để thực hiện các thao tác thêm, sửa, xóa và truy vấn dữ liệu. Việc kiểm thử bằng Postman và trực tiếp trên giao diện web giúp em hiểu rõ luồng xử lý request trong môi trường thực tế.\nBên cạnh kiến trúc Serverless, em tiếp tục tìm hiểu mô hình Event-Driven thông qua Amazon SQS và SNS. Em đã tạo Queue và Topic SNS, xây dựng Lambda để xử lý message từ Queue, cũng như thực hành publish và subscribe để mô phỏng các hệ thống xử lý bất đồng bộ. Qua đó, em hiểu được vai trò của hàng đợi trong việc tách rời các thành phần trong hệ thống, tăng độ ổn định và khả năng mở rộng.\nCuối tuần, em có thêm kiến thức tổng quan về AWS Step Functions và khả năng điều phối các Lambda function trong hệ thống microservices theo từng luồng xử lý logic. Thông qua tuần học này, em đã nắm được nền tảng quan trọng của kiến trúc Serverless, Event-Driven và cách xây dựng các hệ thống backend hiện đại trên AWS với khả năng mở rộng, tiết kiệm chi phí và dễ bảo trì.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.8-week8/",
	"title": "Worklog Tuần 8",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 8: Hiểu và triển khai tự động hóa tạo tài nguyên AWS bằng CloudFormation và AWS CDK. Quản lý cấu hình hệ thống và giám sát phiên làm việc với AWS Systems Manager. Biết cách kết nối các thành phần hạ tầng (EC2, Lambda, API Gateway, S3) theo mô hình Infrastructure as Code. Các công việc triển khai trong tuần Thứ Hoạt động Ngày bắt đầu Ngày kết thúc Tham khảo 2 Tìm hiểu AWS CloudFormation và Cloud9: cấu trúc template JSON/YAML, khái niệm Stack, Drift Detection; Thực hành: tạo CloudFormation template cơ bản sử dụng Cloud9 11/08/2025 11/08/2025 3 Thực hành nâng cao CloudFormation: triển khai Lambda function, tạo Stack, kết nối EC2, ánh xạ các tài nguyên với StackSets, kiểm tra Drift Detection, dọn dẹp tài nguyên 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Tìm hiểu AWS Systems Manager (SSM): Patch Manager, Run Command, Session Manager; Thực hành: tạo EC2 instance, gán IAM Role, cấu hình Patch Manager và Run Command, theo dõi session, dọn dẹp tài nguyên 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Tìm hiểu AWS CDK và triển khai với VS Code: tạo EC2 public, cấu hình môi trường phát triển, tạo ECS cluster, application, API Gateway + Load Balancer, Lambda, S3; triển khai Stack và Nested Stack, dọn dẹp tài nguyên 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Thực hành Session Manager: tạo EC2 private/public, gán IAM Role, kết nối từ máy chủ public đến private, cập nhật IAM Role truy cập S3, tạo S3 bucket và S3 Gateway endpoint, theo dõi session logs, cấu hình Port Forwarding, dọn dẹp tài nguyên 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 8 Trong tuần này, em tập trung học và thực hành các giải pháp Infrastructure as Code và quản lý cấu hình hệ thống:\nAWS CloudFormation \u0026amp; CDK\nHiểu rõ cách tạo, triển khai và quản lý Stack, Nested Stack, StackSets. Thực hành xây dựng template JSON/YAML để tự động tạo Lambda, EC2, S3, API Gateway. Áp dụng CloudFormation Drift Detection để phát hiện sự khác biệt giữa template và tài nguyên thực tế. Sử dụng AWS CDK với VS Code để triển khai hạ tầng linh hoạt, dễ bảo trì và mở rộng. AWS Systems Manager (SSM)\nQuản lý EC2 từ xa thông qua Session Manager, không cần SSH trực tiếp. Cấu hình Patch Manager và Run Command để tự động cập nhật, vận hành EC2. Theo dõi logs session và sử dụng Port Forwarding để truy cập EC2 Private. Quản lý hạ tầng tự động\nKết nối các thành phần hạ tầng (Lambda, API Gateway, EC2, S3) theo kiến trúc IaC. Hiểu luồng triển khai, cập nhật và xóa tài nguyên một cách an toàn. Biết dọn dẹp tài nguyên sau thực hành để tối ưu chi phí và tránh dư thừa. Kỹ năng bổ sung\nLàm quen với Cloud9 để phát triển, chỉnh sửa template CloudFormation và CDK. Hiểu nguyên lý vận hành và giám sát các phiên làm việc EC2 private/public. Có khả năng triển khai hệ thống backend Serverless và containerized với API Gateway và Load Balancer. Qua tuần này, em đã nâng cao kỹ năng tự động hóa hạ tầng AWS, hiểu cách quản lý và theo dõi hệ thống một cách hiệu quả, sẵn sàng triển khai các môi trường phức tạp hơn trong các tuần tiếp theo.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.9-week9/",
	"title": "Worklog Tuần 9",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 9: Làm quen với hệ sinh thái dữ liệu và Machine Learning trên AWS. Hiểu các dịch vụ AI/ML trên AWS và biết cách triển khai các pipeline phân tích dữ liệu. Biết sử dụng các dịch vụ AI sẵn có để xử lý dữ liệu, nhận diện, phân tích và tạo dự đoán. Các công việc cần triển khai Thứ Hoạt động Ngày bắt đầu Ngày kết thúc Tham khảo 2 Tìm hiểu AWS Glue (Crawler), Amazon Athena, Amazon QuickSight, Amazon SageMaker 11/08/2025 11/08/2025 3 Thực hành phân tích dữ liệu: tạo IAM Role \u0026amp; Policy, tạo S3 Bucket, triển khai Glue Crawler, tạo notebook với Glue Studio, phân tích dữ liệu với Athena, trực quan hóa với QuickSight, dọn dẹp tài nguyên 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Thực hành SageMaker: tạo SageMaker Studio, chuẩn bị dataset, phân tích và export dữ liệu lên S3, train và tinh chỉnh mô hình ML, triển khai và đánh giá hiệu suất, tinh chỉnh mô hình tự động, dọn dẹp tài nguyên 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Tìm hiểu Amazon Bedrock: Foundation Model, Bedrock Agents, Knowledge Bases, Bedrock Inference Features 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Tìm hiểu Pre-trained AI Services: Rekognition, Translate, Textract, Transcribe, Polly, Comprehend, Kendra, Lookout, Personalize 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 9 Trong tuần này, em tập trung vào hệ sinh thái dữ liệu và Machine Learning trên AWS, kết hợp thực hành và tìm hiểu các dịch vụ AI sẵn có:\nAWS Glue, Athena và QuickSight\nHiểu cách Glue Crawler tự động phát hiện và catalog dữ liệu từ S3. Sử dụng Athena để query dữ liệu trực tiếp từ S3 với SQL. Trực quan hóa dữ liệu với QuickSight, tạo báo cáo động và dashboard. Amazon SageMaker\nTạo và sử dụng SageMaker Studio để chuẩn bị dataset, train và tune mô hình ML. Hiểu luồng triển khai từ dữ liệu raw tới mô hình huấn luyện, đánh giá và xuất kết quả. Thực hành deploy mô hình và kiểm thử hiệu suất, bao gồm hyperparameter tuning tự động. Amazon Bedrock và Pre-trained AI Services\nNắm được cách sử dụng các Foundation Model, Bedrock Agents, và Knowledge Bases. Hiểu tính năng inference trên Bedrock để triển khai AI nhanh chóng. Thử nghiệm các dịch vụ AI sẵn có: Rekognition: nhận diện hình ảnh/video. Translate \u0026amp; Transcribe: dịch thuật và chuyển giọng nói sang văn bản. Textract \u0026amp; Comprehend: trích xuất dữ liệu và phân tích ngôn ngữ. Polly: tổng hợp giọng nói từ văn bản. Kendra \u0026amp; Lookout: tìm kiếm thông tin và giám sát dữ liệu. Personalize: tạo hệ thống đề xuất thông minh. Kỹ năng bổ sung\nQuản lý IAM Role, Policy và quyền truy cập để bảo mật dữ liệu ML. Kết nối dữ liệu giữa S3, Glue, SageMaker và QuickSight để xây dựng pipeline hoàn chỉnh. Dọn dẹp tài nguyên sau thực hành để tối ưu chi phí. Qua tuần này, em đã có khả năng triển khai pipeline dữ liệu và ML trên AWS, hiểu cách kết hợp các dịch vụ để phân tích, trực quan hóa và triển khai mô hình Machine Learning, đồng thời làm quen với các dịch vụ AI sẵn có để tăng tốc phát triển ứng dụng thông minh.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/2-proposal/",
	"title": "APT Magic – Đề xuất dự án",
	"tags": [],
	"description": "",
	"content": "APT Magic Nền tảng AI Serverless cho tạo ảnh cá nhân hóa và tương tác xã hội 1. Tóm tắt điều hành (Executive Summary) APT Magic là một ứng dụng web AI chạy trên kiến trúc serverless, cho phép người dùng tạo, cá nhân hóa và chia sẻ nội dung nghệ thuật như hình ảnh do AI sinh ra. Nền tảng tích hợp các mô hình AI nền tảng thông qua Amazon Bedrock và cung cấp trải nghiệm web mượt mà với Next.js (SSR) được triển khai trên AWS Amplify.\nPhiên bản MVP tập trung vào khả năng tạo ảnh và chia sẻ theo thời gian thực, trong khi Thiết kế tương lai (Future Design) hướng tới việc mở rộng với SageMaker Inference, Step Functions, và AWS MLOps Pipelines để điều phối mô hình và tự động hóa nâng cao.\nAPT Magic hiện đang được phát triển như một kiến trúc AWS-native hiện đại, tiết kiệm chi phí và bảo mật cao cho nhóm người dùng nhỏ đến trung bình, với định hướng mở rộng lên quy mô doanh nghiệp trong tương lai.\n2. Vấn đề đặt ra (Problem Statement) Vấn đề Phần lớn các nền tảng tạo ảnh AI hiện nay có chi phí cao, phụ thuộc vào các API bên thứ ba thiếu minh bạch và cung cấp khả năng cá nhân hóa hạn chế.\nCác nhà phát triển và nhà sáng tạo thường gặp phải độ trễ cao, thiếu tính minh bạch trong quản lý mô hình, và hạn chế quyền kiểm soát đối với dữ liệu người dùng.\nGiải pháp APT Magic tận dụng kiến trúc AWS Serverless để cung cấp:\nTạo ảnh AI theo thời gian thực thông qua Amazon Bedrock (Stability AI). Xác thực người dùng và quản lý nội dung an toàn với Amazon Cognito và DynamoDB. Xử lý API linh hoạt, mở rộng với AWS Lambda và API Gateway. Phân phối toàn cầu, độ trễ thấp nhờ CloudFront CDN và WAF bảo mật. Trong tương lai, hệ thống sẽ được nâng cấp với Step Functions orchestration, SQS/SNS decoupling, SageMaker Inference Pipelines, và CI/CD tiết kiệm chi phí thông qua CodeBuild, CodePipeline, và CloudFormation, đưa APT Magic trở thành một nền tảng MLOps tự động hóa hoàn chỉnh.\n3. Kiến trúc giải pháp (Solution Architecture) Kiến trúc MVP MVP được xây dựng theo kiến trúc serverless hoàn toàn, tập trung vào khả năng mở rộng, dễ bảo trì và tối ưu chi phí.\nCác dịch vụ AWS cốt lõi:\nRoute53 + CloudFront + WAF — Truy cập toàn cầu an toàn và lưu cache. Amplify (Next.js SSR) — Lưu trữ frontend và server-side rendering. API Gateway + Lambda Functions — Xử lý backend (tạo ảnh, subscription, post API). Amazon Cognito — Xác thực người dùng và kiểm soát truy cập. Amazon S3 + DynamoDB — Lưu trữ dữ liệu và hình ảnh. Amazon Bedrock — Tích hợp mô hình AI nền tảng (Stability AI). Secrets Manager, CloudWatch, CloudTrail — Bảo mật, ghi log và giám sát. Bảo mật\nPrivateLink giúp giao tiếp an toàn giữa Lambda và các dịch vụ backend. WAF + IAM Policies để lọc traffic và kiểm soát truy cập theo vai trò. Thiết kế tương lai (Future Design – Kiến trúc nâng cao) Ở giai đoạn tiếp theo, APT Magic sẽ phát triển thành một nền tảng điều phối AI (AI Orchestration Platform), bổ sung các lớp tự động hóa, khả năng chống lỗi và quản lý vòng đời mô hình.\nCác dịch vụ sẽ bổ sung:\nAWS Step Functions — Điều phối các workflow bất đồng bộ như:\nTạo ảnh AI nhiều bước (kiểm tra prompt → inference → upload kết quả). Xác nhận thanh toán → xử lý mô hình → gửi thông báo. Amazon SQS — Hàng đợi thông điệp cho các tác vụ Lambda bất đồng bộ.\nAmazon SNS — Gửi thông báo sự kiện theo thời gian thực.\nAmazon ElastiCache (Redis) — Giới hạn tốc độ và cache yêu cầu inference.\nAmazon SageMaker Inference — Triển khai các mô hình fine-tune và quản lý endpoint.\nAWS CodePipeline + SageMaker Pipelines — Tự động hóa MLOps: huấn luyện, đánh giá và triển khai.\nAWS PrivateLink + VPC Endpoints — Luồng dữ liệu an toàn giữa Lambda, S3 và SageMaker.\nAWS WAF \u0026amp; Shield Advanced — Bảo vệ DDoS và lọc bảo mật nâng cao.\nCI/CD + MLOps\nCodePipeline + CodeBuild + CloudFormation cho tự động hóa triển khai hạ tầng. 4. Triển khai kỹ thuật (Technical Implementation) Các giai đoạn triển khai Giai đoạn 1 – Triển khai MVP (Hoàn thành / Hiện tại)\nTriển khai Amplify (Next.js SSR) + API Gateway + Lambda. Tích hợp Bedrock Stability AI API. Triển khai CI/CD với CodePipeline + CloudFormation. Kích hoạt xác thực người dùng (Cognito) và lưu trữ dữ liệu (S3 + DynamoDB). Giai đoạn 2 – Mở rộng theo Future Design\nBổ sung Step Functions + SQS/SNS để quản lý workflow AI bất đồng bộ. Thêm ElastiCache để giới hạn request và cache. Tích hợp SageMaker Inference cho mô hình fine-tune. Triển khai SageMaker Pipelines cho huấn luyện và triển khai tự động. Nâng cao bảo mật với Shield Advanced + GuardDuty + PrivateLink. Kết nối GitLab Runner với CodeBuild cho hệ thống CI/CD thống nhất. 5. Lộ trình \u0026amp; Các mốc triển khai (Timeline \u0026amp; Milestones) 6. Ước tính chi phí (AWS Pricing Estimate) Dịch vụ Chi phí ước tính / tháng Ghi chú Lambda + API Gateway $0.50 \u0026lt; 1 triệu lượt gọi Amplify (Next.js SSR) $0.35 Hosting web và build S3 + DynamoDB $0.20 Lưu trữ ảnh và metadata Bedrock Inference $3.00 Dựa trên mức sử dụng (Stability AI) ElastiCache (Tương lai) $1.00 Cache giới hạn tốc độ Step Functions + SQS/SNS $0.60 Điều phối workflow SageMaker Inference (Tương lai) $5.00 Chi phí endpoint CloudWatch + WAF + Shield $1.00 Log và bảo mật Tổng (Ước tính) ~$11.65/tháng Tăng theo mức sử dụng 7. Đánh giá rủi ro (Risk Assessment) Rủi ro Mức độ ảnh hưởng Xác suất Biện pháp Độ trễ khi inference AI Trung bình Cao Dùng ElastiCache + Step Functions Chi phí tăng do gọi mô hình Cao Trung bình Kiểm soát Bedrock, autoscaling SageMaker Lỗi cấu hình CI/CD Trung bình Thấp CloudFormation rollback Lỗ hổng bảo mật Cao Trung bình WAF, GuardDuty, PrivateLink, IAM least privilege Phụ thuộc API bên thứ ba Trung bình Trung bình Fallback Bedrock bằng dữ liệu S3 8. Kết quả kỳ vọng (Expected Outcomes) Kết quả kỹ thuật: Hoàn chỉnh quy trình tạo ảnh AI serverless với CI/CD an toàn. Điều phối module linh hoạt, sẵn sàng cho mở rộng MLOps. Giảm độ trễ và tăng độ ổn định nhờ cache và workflow bất đồng bộ. Giá trị dài hạn: Nền tảng cho hệ sinh thái AI as a Service (AIaaS). Khung MLOps tự động, sẵn sàng huấn luyện lại mô hình. Hạ tầng cloud dùng chung cho các sản phẩm AI trong tương lai. "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "IAM permissions Các quyền cần thêm AdministratorAccess AmazonBedrockFullAccess AWSCodeBuildAdminAccess AWSCodeBuildDeveloperAccess BedrockAgentCoreFullAccess Tạo user và gán quyền Vào IAM → Users → chọn Create user. Thêm các quyền đã liệt kê ở trên. Hoàn tất tạo user và lưu Access Key nếu cần dùng SDK. Tải AWS CLI Tải AWS CLI: AWS CLI Link\nSau đó cài đặt theo hướng dẫn.\nCấu hình UV management 1. Vì sao dùng UV? UV nhanh, nhẹ, quản lý môi trường tốt hơn pip.\n2. Cài đặt UV trên Windows Chạy lệnh:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Thêm UV vào PATH:\n$env:Path = \u0026#34;C:\\Users\\leamo\\.local\\bin;$env:Path\u0026#34; Khởi động lại máy để nhận PATH mới.\n3. Khởi tạo môi trường UV Trong thư mục dự án:\nuv init Sau đó chọn environment trong VS Code.\nKết nối máy với AWS CLI Vào lại IAM để tạo Access Key.\nTạo Access Key và cấu hình AWS CLI Trong user: Security credentials → Create access key Chọn loại Command Line Interface (CLI) Cấu hình AWS CLI Chạy lệnh:\naws configure Nhập lần lượt:\nAWS Access Key ID\nAWS Secret Access Key\nDefault region name (demo: ap-southeast-1)\nDefault output format\njson Khởi chạy AWS CLI AgentCore Chạy:\nuv run which agentcore sau khi chạy sẽ tải các thư viện cần thiết cho AWS Agentcore về máy\nTạo Groq API bạn vào trang Groq và tạo api như hình.Đây là các tool bên ngoài bổ trợ cho Rag của liên kết thông qua AWS Agentcore "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.10-week10/",
	"title": "Worklog Tuần 10",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 10: Hiểu và triển khai giám sát hệ thống với Amazon CloudWatch. Nắm được cách AWS CloudTrail ghi lại hoạt động API và quản lý log theo thời gian thực. Biết cách giám sát ứng dụng serverless và frontend/backend với AWS Amplify. Các công việc cần triển khai Thứ Hoạt động Ngày bắt đầu Ngày kết thúc Tham khảo 2 Tìm hiểu Amazon CloudWatch: Metrics, Logs, Alarms, Events, Dashboards, AWS X-Ray 11/08/2025 11/08/2025 3 Thực hành CloudWatch: tạo IAM Role \u0026amp; Policy, cấu hình EC2, cài đặt CloudWatch Metrics \u0026amp; Logs, tạo Alarm, tạo Dashboard, dọn dẹp tài nguyên 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Tìm hiểu AWS CloudTrail: Trails, Event (Read/Write/All), CloudTrail Insights 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Tìm hiểu AWS Amplify: Frontend, Backend, Storage, Authentication 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Thực hành: Giám sát Lambda với CloudWatch \u0026amp; X-Ray, host source code trên Amplify, tạo custom metric, Alarm, gỡ lỗi logs, giám sát với X-Ray, dọn dẹp tài nguyên 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 10 Trong tuần này, em tập trung vào giám sát hệ thống và quản lý hoạt động trên AWS, kết hợp thực hành với các dịch vụ CloudWatch, CloudTrail và Amplify:\nAmazon CloudWatch\nHiểu và sử dụng Metrics để giám sát tài nguyên EC2, Lambda, DynamoDB. Thiết lập Logs để theo dõi chi tiết các hoạt động ứng dụng và hạ tầng. Tạo Alarms để cảnh báo khi có sự cố hoặc vượt ngưỡng định sẵn. Thiết kế Dashboard tổng quan theo dõi các tài nguyên AWS. Sử dụng X-Ray để debug và phân tích hiệu suất của Lambda và API Gateway. AWS CloudTrail\nTìm hiểu cách ghi lại toàn bộ API call, theo dõi Read/Write events. Sử dụng CloudTrail Insights để phát hiện hành vi bất thường trong hệ thống. Kết hợp với CloudWatch để lập alert và tự động hóa phản hồi khi có sự kiện quan trọng. AWS Amplify\nHost frontend/backend ứng dụng nhanh chóng. Triển khai authentication và storage để tích hợp với Lambda và DynamoDB. Giám sát hoạt động ứng dụng thông qua CloudWatch Logs và X-Ray. Kỹ năng bổ sung\nQuản lý IAM Role \u0026amp; Policy để bảo mật và phân quyền đúng mức. Kết hợp CLI và Console để quản lý đồng thời các tài nguyên AWS. Lập kế hoạch cleanup tài nguyên sau thực hành để tối ưu chi phí. Qua tuần này, em đã có khả năng giám sát hệ thống AWS theo thời gian thực, debug ứng dụng serverless, tạo cảnh báo và dashboard, đồng thời quản lý tài nguyên frontend/backend với Amplify một cách hiệu quả.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.11-week11/",
	"title": "Worklog Tuần 11",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 11: Hiểu quy trình CI/CD và các công cụ hỗ trợ trên AWS. Trải nghiệm và nắm bắt các dịch vụ AI pre-trained của Amazon. Các công việc cần triển khai Thứ Hoạt động Ngày bắt đầu Ngày kết thúc Tham khảo 2 Tham gia Event: Generative AI with Amazon Bedrock + Cập nhật kiến thức về các Foundation Model trên Amazon Bedrock + Hiểu rõ hơn các dịch vụ Pre-trained AI Service 11/08/2025 11/08/2025 3 Tìm hiểu các dịch vụ CI/CD trên AWS: + AWS CodeCommit + AWS CodeBuild + AWS CodeDeploy + AWS CodePipeline 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Tìm hiểu và thực hành AWS Elastic Beanstalk: + Application \u0026amp; Environment + Tạo CloudFormation Stack + Kết nối EC2 instance và cài database + Triển khai và cập nhật ứng dụng trên Elastic Beanstalk + Kiểm tra trạng thái môi trường và truy vấn thông tin máy chủ EC2 + Dọn dẹp tài nguyên 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Thực hành Amazon Polly: + Cài đặt DynamoDB + Khám phá tính năng chuyển văn bản thành giọng nói (TTS) + Tạo Speech và speech marks bằng CLI và SDK Java 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Thực hành Amazon Rekognition: + Tạo Cognito Identity Pool + Phát hiện đối tượng và nhận diện khuôn mặt + Thử nghiệm ứng dụng tìm người + Gắn EBS volume cho EC2 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 11 Trong tuần này, em đã tập trung vào triển khai CI/CD, sử dụng Elastic Beanstalk và trải nghiệm AI Services của AWS, với các kết quả chính như sau:\nCI/CD trên AWS\nHiểu quy trình phát triển phần mềm tự động với CodeCommit, CodeBuild, CodeDeploy, CodePipeline. Biết cách quản lý source code, xây dựng, test và triển khai ứng dụng tự động. AWS Elastic Beanstalk\nTriển khai ứng dụng web nhanh chóng trên môi trường quản lý sẵn. Tạo và cập nhật stack thông qua CloudFormation, kết nối EC2 instance và database. Kiểm tra và giám sát trạng thái môi trường, truy vấn thông tin server. Amazon AI Services\nAmazon Polly: chuyển văn bản thành giọng nói, tạo speech marks, tích hợp với DynamoDB. Amazon Rekognition: phát hiện đối tượng, nhận diện khuôn mặt và thử nghiệm ứng dụng tìm người. Thiết lập Cognito Identity Pool để quản lý quyền truy cập người dùng. Kỹ năng bổ sung\nQuản lý IAM Role \u0026amp; Policy để phân quyền đúng mức cho các dịch vụ. Sử dụng song song Console và CLI để quản lý, kiểm tra và cleanup tài nguyên AWS. Nắm được cách kết hợp các dịch vụ CI/CD, compute và AI để triển khai ứng dụng end-to-end. Qua tuần này, em đã có khả năng triển khai ứng dụng web với CI/CD, trải nghiệm dịch vụ AI pre-trained và quản lý toàn bộ pipeline từ code đến deployment, đồng thời thực hành giám sát và bảo mật tài nguyên AWS.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/1-worklog/1.12-week12/",
	"title": "Worklog Tuần 12",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 12: Hệ thống lại toàn bộ kiến thức về Serverless và AI. Thiết kế chi tiết cơ sở dữ liệu DynamoDB phục vụ cho dự án và hệ thống API. Rà soát lại toàn bộ luồng kết nối giữa các dịch vụ trong hệ thống. Các công việc triển khai trong tuần: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Ôn tập kiến thức và thiết kế cơ sở dữ liệu DynamoDB + Rà soát lại Partition Key (PK) và Sort Key (SK) - Thực hành: + Thiết kế Schema cho các bảng dữ liệu của dự án + Xác định các Access Pattern (cách truy vấn dữ liệu) 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Ôn tập và thiết kế hệ thống API (API Gateway + Lambda): + Rà soát cách triển khai REST API và Lambda Proxy Integration + Liệt kê danh sách các API endpoint cần thiết như POST, GET 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Ôn tập lại quy trình tích hợp AI với Amazon Bedrock + Rà soát các phương thức gọi API của Bedrock + Ôn tập các kỹ thuật Prompt Engineering cơ bản 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Ôn tập về bảo mật và xác thực (Cognito + IAM): + Rà soát luồng xác thực: Người dùng đăng nhập → Nhận Token → Gửi Token lên API Gateway + Ôn lại cách xây dựng IAM Policy theo nguyên tắc cấp quyền tối thiểu 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Ôn tập công tác giám sát và vận hành hệ thống + Rà soát CloudWatch để lên phương án giám sát khi triển khai dự án + Ôn tập cách sử dụng thư viện Boto3 + Lập kế hoạch chi tiết các bước triển khai dự án 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được trong tuần 12: Hoàn thiện thiết kế Schema cho DynamoDB:\nXây dựng cấu trúc bảng dữ liệu rõ ràng, tối ưu cho các nhu cầu truy vấn của hệ thống. Xác định đầy đủ giao diện kết nối giữa các thành phần:\nĐịnh nghĩa rõ ràng input và output cho các hàm Lambda. Nắm vững cách sử dụng thư viện Boto3 để liên kết các dịch vụ AWS trong hệ thống. Xây dựng sẵn chiến lược Prompt (Prompt Strategy) nhằm phục vụ việc kiểm thử và triển khai AI với Bedrock đúng theo yêu cầu.\nHiểu rõ luồng xử lý Identity (danh tính người dùng) từ Cognito qua API Gateway và xuống Lambda.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.3-architecture/5.3.1-agentcore-memory/",
	"title": "Agent core memory",
	"tags": [],
	"description": "",
	"content": "Cấu hình Memory trong AgentCore Để tạo bộ nhớ cho AgentCore, thực hiện theo các bước dưới đây.\n1. Tạo Memory trong Bedrock Vào Bedrock → chọn AgentCore. Chuyển sang tab Memory. Nhấn Create memory. Trong giao diện tạo Memory, bạn sẽ thấy các phần sau:\nMemory name Đặt tên bộ nhớ mà AgentCore sẽ sử dụng.\nShort-term memory (raw event) expiration Số ngày hệ thống lưu lại lịch sử hội thoại chi tiết.\nDemo này có thể để mặc định 90 ngày.\n2. Các loại Memory trong AgentCore 1. Summarization – Tóm tắt hội thoại Chức năng: Tóm tắt nội dung hội thoại sau khi kết thúc hoặc theo chu kỳ.\nTác dụng: Giữ lại ngữ cảnh dài hạn mà không chiếm nhiều bộ nhớ.\nVí dụ:\nBạn chat 100 câu về lỗi AWS CLI → lần sau Agent nhớ:\n“Người dùng đang gặp lỗi kết nối AWS CLI.”\n2. Semantic Memory – Bộ nhớ ngữ nghĩa Chức năng: Lưu trữ các facts/kiến thức quan trọng, độc lập với ngữ cảnh.\nTác dụng: Dùng để trả lời các câu hỏi liên quan đến thông tin đã nói trước đó.\nVí dụ:\n“Dự án A dùng Python 3.9.”\nLần sau hỏi lại → Agent trả lời ngay Python 3.9.\n3. User Preferences – Sở thích người dùng Chức năng: Tự nhận biết thói quen và phong cách của người dùng.\nTác dụng: Cá nhân hóa cách phản hồi.\nVí dụ:\nBạn hay yêu cầu:\n“Hãy trả lời ngắn gọn.”\nAgent sẽ tự động trả lời đúng ý mỗi lần.\n4. Episodes – Bộ nhớ tình huống Chức năng: Lưu chuỗi sự kiện, phân tích thành công/thất bại qua Reflections.\nTác dụng: Giúp Agent học từ kinh nghiệm.\nVí dụ:\nLần trước đặt vé bị lỗi vì thiếu ngày tháng → Agent nhớ.\nLần sau nó sẽ hỏi ngày tháng trước.\n3. Loại Memory dùng trong Demo Trong phần demo, chỉ cần dùng loại:\nSummarization Chọn Summarization rồi nhấn Create để hoàn tất.\n4. Cập nhật Memory ID trong Python Sau khi tạo Memory, bạn sẽ nhận được Memory ID.\nThêm vào file Python như sau:\n# AgentCore Memory Configuration REGION = \u0026#34;ap-southeast-1\u0026#34; MEMORY_ID = \u0026#34;memory_j98zj-4LFDxqB2o1\u0026#34; GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) lưu ý thay đổi Id Memory và Region theo đúng cái bạn đã tạo\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/3-blogstranslated/",
	"title": "Các bài blogs đã dịch",
	"tags": [],
	"description": "",
	"content": "Tại đây sẽ là phần liệt kê, giới thiệu các blogs mà các bạn đã dịch. Ví dụ:\nBlog 1 - Getting started with healthcare data lakes: Using microservices Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 2 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 3 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 4 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 5 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 6 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.3-architecture/5.3.3-chunking/",
	"title": "Chunking &amp; Embedding",
	"tags": [],
	"description": "",
	"content": "Lý do Chunking Tài liệu/FAQ thường dài; để tính embedding hiệu quả và tối ưu truy vấn tương đồng, cần chia văn bản lớn thành các đoạn nhỏ (chunk).\nLợi ích Giảm loss of context khi embed Retrieval chính xác hơn bằng vector similarity Tối ưu hiệu suất khi tìm kiếm Phù hợp với giới hạn token của model embedding Chiến lược Chunking Trong code sử dụng RecursiveCharacterTextSplitter:\nsplitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) Tham số Chunking Tham số Giá trị Ý nghĩa chunk_size 500 Kích thước mỗi chunk (characters) chunk_overlap 0 Không có phần chồng lấn giữa các chunk Gợi ý Tối ưu Kích thước chunk hợp lý: 500–1000 tokens (tùy model embedding) Chunk overlap: Nếu cần context liên tục, set overlap 50-100 characters Trade-off: Chunk nhỏ → chính xác cao nhưng nhiều vectors; Chunk lớn → ít vectors nhưng có thể mất ngữ cảnh Tạo Embedding và Vector Store Khởi tạo Embedding Model emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) Model được chọn: all-MiniLM-L6-v2\nLightweight và nhanh Phù hợp cho tiếng Anh Kích thước embedding: 384 dimensions Tạo FAISS Vector Store faq_store = FAISS.from_documents(chunks, emb) FAISS (Facebook AI Similarity Search) cung cấp:\nTìm kiếm vector nhanh chóng Hiệu quả với datasets lớn Hỗ trợ nhiều thuật toán index Truy vấn Vector Store results = faq_store.similarity_search(query, k=3) Tham số:\nquery: Câu hỏi người dùng k=3: Trả về top 3 chunks có độ tương đồng cao nhất Ghi chú Quan trọng Cập nhật Dữ liệu Nếu dữ liệu thay đổi (add/update documents), cần: Re-embed toàn bộ hoặc Incremental update vector store Chọn Embedding Model Trade-off cần cân nhắc:\nTiêu chí Lightweight Model Heavy Model Tốc độ Nhanh Chậm Độ chính xác Tốt Rất tốt Chi phí Thấp Cao Use case FAQ, chatbot Research, legal Model tiếng Việt Nếu cần hỗ trợ tiếng Việt tốt hơn:\nkeepitreal/vietnamese-sbert sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 Code Hoàn chỉnh from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.vectorstores import FAISS # Load documents docs = load_faq_csv() # Chunking splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) # Embedding emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) # Vector Store faq_store = FAISS.from_documents(chunks, emb) # Query query = \u0026#34;Làm thế nào để đổi mật khẩu?\u0026#34; results = faq_store.similarity_search(query, k=3) Checklist Triển khai Chuẩn bị documents (CSV, JSON, text files) Chọn chunk_size phù hợp (test với 500, 750, 1000) Chọn embedding model (tiếng Anh hoặc đa ngôn ngữ) Tạo và lưu FAISS index Test retrieval với các query mẫu Monitor và điều chỉnh k value "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.3-architecture/5.3.4-agent-core/",
	"title": "Code AgentCore",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Giải thích hàm xử lý trong agent, cách agent gọi Groq, và cách gắn memory ID vào agent để duy trì context lâu dài.\nEntrypoint AgentCore Đăng ký Entrypoint Hàm entrypoint được đăng ký bằng decorator @app.entrypoint:\n@app.entrypoint def agent_invocation(payload, context): \u0026#34;\u0026#34;\u0026#34; Main entry point for agent execution Args: payload: Input data containing prompt, actor_id, thread_id context: Runtime metadata \u0026#34;\u0026#34;\u0026#34; # ... xử lý logic Input Parameters payload: Chứa input từ client prompt: Câu hỏi người dùng actor_id: ID người dùng (optional) thread_id: ID phiên chat (optional) context: Metadata runtime từ AgentCore Cấu hình Memory Lấy Actor ID và Thread ID actor_id = payload.get(\u0026#34;actor_id\u0026#34;, \u0026#34;default-user\u0026#34;) thread_id = payload.get( \u0026#34;thread_id\u0026#34;, payload.get(\u0026#34;session_id\u0026#34;, \u0026#34;default-session\u0026#34;) ) config = { \u0026#34;configurable\u0026#34;: { \u0026#34;thread_id\u0026#34;: thread_id, \u0026#34;actor_id\u0026#34;: actor_id } } Memory ID MEMORY_ID là identifier duy nhất cho memory storage:\nMEMORY_ID = \u0026#34;memory_j98zj-4LFDxqB2o1\u0026#34; # Khởi tạo memory components checkpointer = AgentCoreMemorySaver(memory_id=MEMORY_ID) store = AgentCoreMemoryStore(memory_id=MEMORY_ID) Vai trò của Memory ID:\nTrỏ tới namespace lưu trữ memory (S3/DynamoDB/Bedrock) Đảm bảo nhất quán giữa các instance Cho phép chia sẻ memory pool nếu cần Tích hợp Memory vào Agent agent = create_agent( model=llm, tools=tools, checkpointer=checkpointer, store=store, middleware=[MemoryMiddleware()], system_prompt=system_prompt, ) Memory Middleware Cấu trúc Middleware MemoryMiddleware có hai hook chính:\n1. Pre-Model Hook Chạy trước khi gọi model:\ndef pre_model_hook(self, state, config): \u0026#34;\u0026#34;\u0026#34; Xử lý trước khi gọi LLM - Lưu user message vào memory - Load preferences và past memories - Enrich context \u0026#34;\u0026#34;\u0026#34; actor_id = config[\u0026#34;configurable\u0026#34;][\u0026#34;actor_id\u0026#34;] thread_id = config[\u0026#34;configurable\u0026#34;][\u0026#34;thread_id\u0026#34;] # Lấy messages hiện tại messages = state.get(\u0026#34;messages\u0026#34;, []) # Lưu user message for msg in messages: if msg.type == \u0026#34;human\u0026#34;: namespace = (actor_id, thread_id) store.put( namespace, str(uuid.uuid4()), {\u0026#34;message\u0026#34;: msg} ) # Load memories và preferences memories = store.search(namespace, query=msg.content) # Append vào context... 2. Post-Model Hook Chạy sau khi model trả về:\ndef post_model_hook(self, state, config): \u0026#34;\u0026#34;\u0026#34; Xử lý sau khi LLM response - Lưu AI message vào long-term memory - Update statistics \u0026#34;\u0026#34;\u0026#34; messages = state.get(\u0026#34;messages\u0026#34;, []) # Lưu AI response for msg in messages: if msg.type == \u0026#34;ai\u0026#34;: namespace = (actor_id, thread_id) store.put( namespace, str(uuid.uuid4()), {\u0026#34;message\u0026#34;: msg} ) Mục đích Memory Middleware Duy trì lịch sử hội thoại dài hạn Personalization: Nhớ preferences người dùng Context enrichment: Thêm thông tin relevant từ quá khứ Analytics: Track conversation patterns Agent Execution Flow 1. Invoke Agent query = payload.get(\u0026#34;prompt\u0026#34;, \u0026#34;\u0026#34;) result = agent.invoke( {\u0026#34;messages\u0026#34;: [(\u0026#34;human\u0026#34;, query)]}, config=config ) 2. Agent Processing Pipeline 1. Pre-Model Hook ├── Lưu user message ├── Load memories └── Enrich context 2. Tool Decision ├── search_faq() ├── search_detailed_faq() └── reformulate_query() 3. Call LLM (Groq) ├── Send prompt + context └── Receive response 4. Post-Model Hook ├── Lưu AI response └── Update memory 3. Trích xuất Kết quả messages = result.get(\u0026#34;messages\u0026#34;, []) answer = messages[-1].content if messages else \u0026#34;No response generated\u0026#34; return { \u0026#34;result\u0026#34;: answer, \u0026#34;actor_id\u0026#34;: actor_id, \u0026#34;thread_id\u0026#34;: thread_id } Tool Functions Agent có thể sử dụng các tools để tăng độ chính xác:\nĐịnh nghĩa Tool @tool def search_faq(query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Search FAQ database for quick answers Args: query: User question Returns: Relevant FAQ content \u0026#34;\u0026#34;\u0026#34; results = faq_store.similarity_search(query, k=3) return \u0026#34;\\n\u0026#34;.join([doc.page_content for doc in results]) Các Tool Khác @tool def search_detailed_faq(query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Tìm kiếm chi tiết với k=5\u0026#34;\u0026#34;\u0026#34; results = faq_store.similarity_search(query, k=5) return format_results(results) @tool def reformulate_query(original_query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Cải thiện câu query để tìm kiếm tốt hơn\u0026#34;\u0026#34;\u0026#34; # Logic reformulation... return improved_query Vai trò Tools Tách Retrieval khỏi Generation (R vs G trong RAG) Cải thiện độ chính xác: Agent quyết định khi nào dùng tool Flexible workflow: Có thể chain nhiều tools Example Payload \u0026amp; Response Input Payload { \u0026#34;prompt\u0026#34;: \u0026#34;Làm thế nào để reset mật khẩu?\u0026#34;, \u0026#34;actor_id\u0026#34;: \u0026#34;user_12345\u0026#34;, \u0026#34;thread_id\u0026#34;: \u0026#34;session_abc\u0026#34; } Output Response { \u0026#34;result\u0026#34;: \u0026#34;Để reset mật khẩu, bạn có thể:\\n1. Vào Settings \u0026gt; Security\\n2. Chọn \u0026#39;Reset Password\u0026#39;\\n3. Nhập email để nhận link reset\u0026#34;, \u0026#34;actor_id\u0026#34;: \u0026#34;user_12345\u0026#34;, \u0026#34;thread_id\u0026#34;: \u0026#34;session_abc\u0026#34; } Best Practices Memory Management Consistent Memory ID: Dùng cùng MEMORY_ID cho cùng một service Namespace isolation: Sử dụng (actor_id, thread_id) để tách biệt conversations Cleanup policy: Định kỳ xóa old memories để tối ưu storage Error Handling @app.entrypoint def agent_invocation(payload, context): try: # ... main logic result = agent.invoke(...) return {\u0026#34;result\u0026#34;: answer, ...} except Exception as e: logger.error(f\u0026#34;Agent error: {e}\u0026#34;) return { \u0026#34;error\u0026#34;: str(e), \u0026#34;actor_id\u0026#34;: actor_id, \u0026#34;thread_id\u0026#34;: thread_id } Performance Async tools nếu có I/O operations Cache embeddings để tránh recompute Batch processing cho multiple queries Checklist Triển khai Định nghĩa MEMORY_ID duy nhất Implement AgentCoreMemorySaver và AgentCoreMemoryStore Tạo MemoryMiddleware với pre/post hooks Định nghĩa tools (@tool decorator) Implement entrypoint function Test với payload mẫu Verify memory persistence Monitor tool usage và LLM calls Kết quả: Một AI agent thông minh, có khả năng nhớ context và tự động chọn công cụ phù hợp.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.3-architecture/5.3.2-groq-api/",
	"title": "Gọi Groq API",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Sử dụng thư viện Groq (ở đây là ChatGroq / init_chat_model với model_provider=\u0026quot;groq\u0026quot;) để gọi model OpenAI (Groq-hosted).\nCấu hình trong Code Trong code demo:\nLấy API Key từ Environment GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) Biến GROQ_API_KEY lấy API key từ environment variable.\nKhởi tạo Model llm = init_chat_model( model=\u0026#34;openai/gpt-oss-20b\u0026#34;, model_provider=\u0026#34;groq\u0026#34;, api_key=GROQ_API_KEY ) Tích hợp vào Agent Agent gọi LLM thông qua create_agent(...) với tham số model=llm:\nagent = create_agent( model=llm, tools=tools, checkpointer=checkpointer, store=store, middleware=[MemoryMiddleware()], system_prompt=system_prompt, ) Luồng xử lý Agent → Groq API → Model Inference → Response "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.3-architecture/",
	"title": "Kiến trúc mô hình Rag triển khai trên AWS Agent core",
	"tags": [],
	"description": "",
	"content": "Sử dụng Gateway endpoint Trong phần này, chúng ta sẽ tìm hiểu cách tích hợp Groq để gọi model OpenAI-compatible và cách chunking dữ liệu cho RAG.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/4-eventparticipated/",
	"title": "Các events đã tham gia",
	"tags": [],
	"description": "",
	"content": "Trong quá trình thực tập, em đã tham gia 2 events, với mỗi event là một trải nghiệm đáng nhớ với những kiến thức mới, hay và bổ ích, cùng với đó là nhứng món quà và những khoảnh khắc rất tuyệt vời.\nEvent 1 Tên sự kiện: Cloud Day\nThời gian: 09:00 ngày 18/09/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 2 Tên sự kiện: Generative AI with Amazon Bedrock\nThời gian: 08:30 ngày 15/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/5.4-agent-core-run/",
	"title": "Run Agent Core",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần này, bạn sẽ học cách triển khai và gọi AWS Agent Core từ máy local Tại sao nên sử dụng AWS CLI: AWS CLI có thể giúp bạn truy cập và cấu hình set up được Agent Core từ máy của mình, linh hoạt và tiện lợi "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Xây dựng RAG Agent với Groq API và AgentCore Memory Tổng quan Trong workshop này, chúng ta sẽ xây dựng một RAG (Retrieval-Augmented Generation) Agent hoàn chỉnh với khả năng:\nGọi Groq API để sử dụng LLM models với hiệu năng cao Chunking \u0026amp; Embedding documents để tối ưu vector search AgentCore Memory để duy trì context lâu dài qua các phiên chat Tool Integration để agent có thể tự động search FAQ và reformulate queries AgentCore cung cấp framework để xây dựng AI agents với memory persistence, middleware hooks, và tool orchestration - cho phép agent \u0026ldquo;nhớ\u0026rdquo; lịch sử hội thoại và personalize responses.\nNội dung Tổng quan về Workshop Chuẩn bị Kiến trúc 5.3.1. Gọi Groq API 5.3.2. Chunking \u0026amp; Embedding 5.3.3. Code Handler AgentCore Chạy Agent Core Dọn dẹp tài nguyên Tech Stack Component Technology LLM Provider Groq API (OpenAI models) Embedding Model HuggingFace (all-MiniLM-L6-v2) Vector Store FAISS Agent Framework LangChain + AgentCore Memory Backend AgentCore Memory Store Text Splitting RecursiveCharacterTextSplitter Điều kiện tiên quyết Python 3.8+ Groq API Key Kiến thức cơ bản về RAG và LLM Hiểu biết về vector embeddings Kết quả mong đợi Sau workshop, bạn sẽ có:\nAgent có khả năng trả lời FAQ dựa trên vector search\nMemory system để nhớ preferences và context người dùng\nTool orchestration để agent tự quyết định khi nào dùng tool\nProduction-ready code với error handling và logging\nBắt đầu: 5.1. Tổng quan về Workshop\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/6-self-evaluation/",
	"title": "Tự đánh giá",
	"tags": [],
	"description": "",
	"content": "Trong suốt thời gian thực tập tại [Tên công ty/tổ chức] từ [ngày bắt đầu] đến [ngày kết thúc], tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã học ở trường vào môi trường làm việc thực tế.\nTôi đã tham gia [mô tả ngắn gọn dự án hoặc công việc chính], qua đó nâng cao kỹ năng [liệt kê kỹ năng: lập trình, phân tích, viết báo cáo, giao tiếp…].\nVề tác phong, tôi luôn cố gắng hoàn thành nhiệm vụ đúng hạn, tuân thủ nội quy và chủ động trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nDưới đây là đánh giá tự nhận của tôi dựa trên các tiêu chí chính:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Nắm vững kiến thức chuyên ngành, áp dụng thực tế và sử dụng công cụ hiệu quả ✅ ☐ ☐ 2 Khả năng học hỏi Tiếp nhận kiến thức mới nhanh chóng và hiệu quả ✅ ☐ ☐ 3 Chủ động Tự đề xuất, nhận nhiệm vụ và giải quyết vấn đề mà không cần hướng dẫn liên tục ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng và kết quả ✅ ☐ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy và quy trình làm việc ☐ ✅ ☐ 6 Tính cầu tiến Sẵn sàng tiếp nhận phản hồi, cải thiện hiệu suất và phát triển bản thân ☐ ✅ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc và trao đổi thông tin rõ ràng ☐ ✅ ☐ 8 Hợp tác nhóm Tham gia nhóm hiệu quả, phối hợp với đồng nghiệp tốt ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác và môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp sáng tạo và thực tế ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Góp phần hiệu quả vào dự án, sáng kiến cải tiến, được team ghi nhận ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về quá trình thực tập ✅ ☐ ☐ Cần cải thiện Rèn luyện tính kỷ luật, tuân thủ nghiêm túc nội quy công ty hoặc tổ chức. Nâng cao kỹ năng tư duy và giải quyết vấn đề. Cải thiện giao tiếp trong công việc và xử lý tình huống thực tế. "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/7-feedback/",
	"title": "Chia sẻ &amp; Đóng góp ý kiến",
	"tags": [],
	"description": "",
	"content": " Tại đây bạn có thể tự do chia sẻ những trải nghiệm cá nhân khi tham gia chương trình First Cloud Journey, đồng thời đóng góp ý kiến giúp team FCJ cải thiện những điểm còn hạn chế theo các hạng mục sau:\nNhận xét chung 1. Môi trường làm việc\nMôi trường làm việc thân thiện và cởi mở. Các thành viên FCJ luôn sẵn sàng hỗ trợ khi gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn. Có thể bổ sung thêm một số hoạt động giao lưu hoặc team bonding để mọi người hiểu nhau hơn.\n2. Hỗ trợ từ mentor / team admin\nMentor hướng dẫn chi tiết, giải thích rõ ràng khi mình chưa hiểu và luôn khuyến khích đặt câu hỏi. Team admin hỗ trợ thủ tục, tài liệu và tạo điều kiện để công việc diễn ra thuận lợi. Mình đánh giá cao việc mentor cho phép thử và tự xử lý vấn đề thay vì chỉ đưa đáp án.\n3. Phù hợp với chuyên ngành học\nCông việc được giao phù hợp với kiến thức đã học, đồng thời mở rộng những lĩnh vực mới chưa từng tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, làm việc nhóm, và giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp định hướng tốt hơn cho sự nghiệp.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí. Điều này giúp mình cảm thấy là một phần của tập thể, dù chỉ là thực tập sinh.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty hỗ trợ phụ cấp thực tập và linh hoạt về thời gian khi cần thiết. Tham gia các buổi đào tạo nội bộ cũng là một điểm cộng lớn.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập là gì? Điều bạn nghĩ công ty cần cải thiện cho thực tập sinh sau? Nếu giới thiệu cho bạn bè, bạn có khuyên họ thực tập ở đây không? Vì sao? Đề xuất \u0026amp; mong muốn Có đề xuất gì để cải thiện trải nghiệm thực tập không? Bạn có muốn chương trình này tiếp tục trong tương lai không? Góp ý khác (tự do chia sẻ): "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]