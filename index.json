[
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Trang Hoang Khang\nPhone Number: 0837525888\nEmail: khangthse182228@fpt.edu.vn\nUniversity: FPT University\nMajor: Artificial Intelligence\nStudent ID: SE182228\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How a customer reduced storage TCO by 28% with Amazon FSx for NetApp ONTAP Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.4-agent-core-run/5.4.2-call-agentcore/",
	"title": "Calling AgentCore",
	"tags": [],
	"description": "",
	"content": "Simple Demo with AgentCore 1. Send the first question Use the command:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Tell me about roaming activations\u0026#39;}\u0026#34; The Agent will respond based on the data you deployed (database + logic in your code).\n2. Test memory between invocations (session) After the first question, send another related one — for example:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;Activate it for Vietnam\u0026#39;}\u0026#34; Then ask:\nagentcore invoke \u0026#34;{\u0026#39;prompt\u0026#39;: \u0026#39;which country was i referring to\u0026#39;}\u0026#34; If the Agent responds correctly and remembers the previous information → this confirms the Memory is working and AgentCore is maintaining context across invocations.\nAgentCore behaves as expected in this demo.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.4-agent-core-run/5.4.1-run-agentcore/",
	"title": "Configure &amp; Deploy AgentCore",
	"tags": [],
	"description": "",
	"content": "Getting Started with AgentCore Configure First, push your local code to AWS AgentCore using the command:\nagentcore configure -e ./{your_python_file.py} 1. Agent Name Enter a name for your Agent.\n2. Configuration File Press Enter to use the default configuration file pyproject.toml.\n3. Deployment Configuration Select 2 – Deploy using Docker, allowing AgentCore to automatically build and manage your Docker image.\n4. Execution Role Keep the default setting and let AWS create the IAM Role automatically.\n5. ECR Repository Press Enter to let AWS create the ECR repository for storing the Docker image.\n6. Authorization Configuration Choose No for OAuth. The Agent will only allow access via AWS IAM Access Key \u0026amp; Secret Key.\n7. Request Header Allowlist Press Enter to use the default allowlist configuration.\nResult Once this step is completed, your code has been successfully uploaded to AgentCore.\nLaunch the Agent Use the command below to start the Agent with your API Key (using GROQ):\nagentcore launch --env GROQ_API_KEY=your_api_key_here When the terminal shows Running, your Agent is successfully running.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Cloud Day” Event Objectives Introduce the trend of AI development in Vietnam and its economic potential. Present the evolution from Generative AI to Agentic AI. Showcase AWS solutions like Amazon Bedrock, AgentCore, and SageMaker Unified Studio for building, deploying, and operating AI agents. Speakers Key Highlights Impact of AI on Vietnam\u0026rsquo;s Economy AI could contribute USD 120–130 billion to Vietnam’s GDP by 2040 (~25%). The AI market is valued at USD 750 million, growing 15–18% annually. Vietnam has 765 AI startups, ranking 2nd in ASEAN. Great potential, but still in an early stage — requires infrastructure, talent, and policy development. Evolution: From AI to Agentic AI Generative AI Assistants → Generative AI Agents → Agentic AI Systems. AI systems are becoming less dependent on human supervision. Multi-agent systems allow agents to collaborate to solve complex tasks. The trend is toward higher automation and lower human intervention. Applications of Agentic AI in Organizations Improve workplace productivity, automate workflows, and drive innovation. By 2028, 33% of enterprise applications will integrate Agentic AI. About 15% of daily business decisions will be automated through AI agents. Amazon Bedrock – The All-in-One AI Platform Offers diverse foundation models from leading providers. Enables customization with private data, ensuring security and cost control. Integrates Responsible AI checks for safe and ethical use. Supports fast, scalable, and secure agent deployment and management. Amazon Bedrock AgentCore A secure, scalable environment for building and running AI Agents. Supports frameworks such as LangChain, CrewAI, LangGraph, and Strands Agents. Manages short-term and long-term memory and supports semantic search. Allows easy integration and tool discovery for developers. Data \u0026amp; AI Infrastructure Introduces Amazon SageMaker Unified Studio – the central hub for data, analytics, and AI development. Integrates tightly with: Amazon Redshift, Athena, EMR, Glue – for data processing and storage. Amazon QuickSight – for data visualization. Amazon Bedrock – for GenAI development. Supports Zero-ETL integration between S3 data lakes and Redshift data warehouses. Data Lakehouse Concept Supports multiple storage layers: S3 Tables, Redshift Managed Storage. Connects various data sources: Aurora, DynamoDB, MSK, Kinesis, OpenSearch, Salesforce, SAP, Facebook Ads, and more. Key Takeaways On AI and Cloud Mindset Understood that Agentic AI is the next evolution of Generative AI. Realized that AI Agents are not just chatbots but systems capable of taking autonomous actions and decisions. Learned how AWS Bedrock provides a foundation for building enterprise-level AI systems. Recognized the strategic value of AI agents in enterprise automation and innovation. On Technical Architecture Learned the relationships between Bedrock – SageMaker – Redshift – S3 in a unified AI ecosystem. Understood how AWS manages memory, tool discovery, and observability for AI agents. Applying to Work Apply Amazon Berock to an existing project: Use Amazon Titan Embeddings to create an embedding Experiment with Zero-ETL integration between Amazon Redshift and Aurora/DynamoDB. Evaluate Amazon Bedrock AgentCore for automating workflows with intelligent agents (beyond Lambda-based implementations). Event Experience Attending Cloud Day was an incredibly insightful experience that gave me a clearer view of how businesses leverage AI to modernize systems and enhance productivity.\nLearning from Experts AWS experts deeply explained Agentic AI and how it differs from traditional Generative AI. Through Amazon’s real-world examples, I gained a deeper understanding of multi-agent system design and workflow optimization. Hands-on Technical Insights Explored the technical operation of Amazon Bedrock AgentCore, including memory handling and tool integration. Observed how S3 – Redshift – SageMaker integrate seamlessly for contextual AI data retrieval. Learned the fundamentals of Lakehouse architecture and Zero-ETL data flow in practice. Using Modern Tools Discovered how to quickly deploy Agentic AI systems on AWS Bedrock with security, scalability, and reliability. Lessons learned Agentic AI represents a strategic step toward full enterprise automation. Modern AI infrastructure must be built on data-driven and cloud-native architecture. AWS is leading the ecosystem with comprehensive tools such as Bedrock and SageMaker. Understood the importance of AI agents in enhancing both automation and innovation in modern businesses. Some event photos Overall, the event not only provided valuable technical knowledge but also reshaped my mindset on system modernization, design thinking, and effective collaboration between teams.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Report “Generative AI with Amazon Bedrock” Purpose of the Event Provide foundational knowledge about Generative AI and how it differs from traditional Machine Learning. Detailed description of Amazon Bedrock and Foundation Models. Technical guidance on RAG (Retrieval Augmented Generation) to build intelligent and accurate AI applications while reducing hallucinations. Introduction to the ecosystem of specialized AI services on AWS. Speakers Lam Tuan Kiet - Sr DevOps Engineer, FPT Software. Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud. Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey. Key Highlights The shift: Traditional ML vs Foundation Models Traditional ML Models: Specialized for specific tasks, require labeled data, and need separate training/deployment pipelines for each purpose. Foundation Models (FM): Trained on massive amounts of unstructured data, capable of adapting to various tasks such as text generation, summarization, Q\u0026amp;A, and chatbot applications. The AI Ecosystem on AWS Amazon Bedrock: A hub of leading foundation models from AWS partners (AI21 Labs, Anthropic, Cohere, Meta, Stability AI,\u0026hellip;) and Amazon models. AWS Specialized AI Services: “Instant-use” AI services optimized for specific tasks without requiring model training: Amazon Rekognition: Object detection, face recognition, emotion detection, celebrity recognition, video analysis – $0.001/image for the first 1M images. Amazon Translate: Real-time multilingual text translation with high accuracy and natural tone. Amazon Textract: Extracts structured information (tables, forms) from scanned documents or PDFs. Amazon Transcribe: Converts speech to text. Amazon Polly: Converts text to speech. Amazon Comprehend: Sentiment analysis, keyword extraction, and topic classification. Amazon Kendra: Natural-language search across internal enterprise documents. Amazon Lookout: Detect anomalies in production lines or industrial machinery for predictive maintenance. Amazon Personalize: Real-time recommendation system powered by machine learning. Prompting Technique: Chain of Thought (CoT) Comparison between Standard Prompting and Chain-of-Thought Prompting. CoT guides the model to reason step-by-step for complex logic problems, significantly improving accuracy compared to simply requesting the final result. RAG (Retrieval Augmented Generation) – Technical Core Problem: Addressing hallucinations and lack of updated knowledge in LLMs. Solution: Combine Retrieval from an external Knowledge Base with the Generation capability of LLMs. Data Ingestion Process: Raw Data (New data) → Chunking. Processed through an Embeddings model (e.g., Amazon Titan Text Embeddings V2.0). Stored as vectors in a Vector Store (OpenSearch Serverless, Pinecone, Redis\u0026hellip;). RetrieveAndGenerate API: Manages the entire pipeline from user input → embedding query → retrieving relevant data → augmenting the prompt → generating the final response. What I Learned AI and Cloud Mindset Understand when to use Specialized AI Services for fast, specific tasks and when to use Bedrock/GenAI for creative and complex workloads. Master the design mindset of RAG systems: not just calling LLM APIs, but managing data and vectorization to provide accurate context for AI outputs. Technical Architecture The Chain of Thought technique is key to optimizing model output without fine-tuning. Deep understanding of Amazon Titan Embeddings V2.0 and its role in converting multilingual text into vectors (supports 100+ languages, flexible vector sizes 256/512/1024). Application to Work Applying Amazon Bedrock to the current project: Amazon Rekognition: Identify food items from images to auto-fill calorie information. Amazon Comprehend: Analyze text to standardize food names and store calorie data. Experiment with RAG in the ongoing project. Use Bedrock Agents to orchestrate tasks such as querying food items from a vector store, calculating calorie goals, and generating daily meal plans. Experience at the Event Joining the workshop “Generative AI with Amazon Bedrock” provided a very practical perspective on building modern AI applications, from foundational theory to real-world implementation.\nHands-on Knowledge from Experts Speakers clearly explained the data flow in a RAG system, helping me visualize the “black box” behind modern chatbot applications. The distinction between Traditional ML and Generative AI helped reshape my strategy for choosing technologies in future projects. Technology Experience Impressed by RetrieveAndGenerate API in Bedrock as it eliminates much manual work in linking Vector Stores with LLMs. Saw the power of Amazon Titan Embedding in supporting multilingual applications, making it very suitable for the Vietnamese market. Key Takeaways RAG is the new standard: For AI to work in enterprises, RAG is essential for accuracy and data security. Full ecosystem: AWS provides everything from infrastructure (Vector Store) to models (Bedrock) and application-level orchestration (Agents), significantly accelerating implementation. Some Photos from the Event Overall, the event not only delivered technical knowledge but also helped reshape my mindset about application design, system modernization, and effective collaboration among teams.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction to the Workshop This workshop provides a step-by-step guide to setting up IAM, AWS CLI, UV, Groq API, deploying RAG source code integrated with Groq LLM into AWS AgentCore, and finally publishing the API through AWS Gateway. Workshop Objectives \u0026ldquo;How to call APIs\u0026rdquo; — understand how to call external APIs outside AWS AgentCore. \u0026ldquo;Chunking\u0026rdquo; — learn how to split data for RAG so it can retrieve information optimally. \u0026ldquo;Adding memory to RAG\u0026rdquo; — explore how the RAG Agent can remember each piece of data during interactions with users. \u0026ldquo;Deploy AWS AgentCore\u0026rdquo; — understand how to deploy AWS AgentCore. \u0026ldquo;Publish API\u0026rdquo; — learn how to call AgentCore through an API. ![overview](ảnh kiến trúc mô hìn)\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "This page summarizes my internship work at First Cloud Journey (FCJ). The internship program lasted 12 weeks, during which I sequentially completed weekly objectives and tasks to gain a solid understanding of core AWS services, implement practical projects, and become familiar with AWS Cloud ecosystems.\nOver the course of 12 weeks, my main activities included:\nWeek 1: Getting Started with AWS and Core Services\nWeek 2: Exploring Networking and Related AWS Services\nWeek 3: Deploying and Managing EC2, EBS, and Security Groups\nWeek 4: Introduction to RDS and NoSQL DynamoDB\nWeek 5: Using Python SDK (Boto3) to Manage AWS Resources\nWeek 6: Learning Amazon ElastiCache and Optimizing Data Access\nWeek 7: Deploying Serverless Architecture with Lambda, API Gateway, SQS, and SNS\nWeek 8: Automating Resource Provisioning with CloudFormation, CDK, and Managing Systems with SSM\nWeek 9: Exploring AWS Data Ecosystem and Machine Learning: Glue, Athena, QuickSight, SageMaker\nWeek 10: System Monitoring with CloudWatch, CloudTrail, and Using AWS Amplify\nWeek 11: Understanding CI/CD with AWS Code Series and Exploring AI Services like Polly, Rekognition\nWeek 12: Serverless \u0026amp; AI Review, DynamoDB Schema Design, API Integration, Security, and Monitoring\nGeneral Remarks Throughout the 12-week internship, I have:\nAccumulated a solid foundational knowledge of AWS, covering Compute, Storage, Networking, Databases, and supporting services. Learned to deploy and manage resources effectively, combining Console and CLI usage. Practiced projects ranging from Serverless, CloudFormation, CDK, EC2, RDS, DynamoDB to AI and Machine Learning services. Gained experience in a real-world environment at FCJ, collaborating with team members, and understanding rules, regulations, and internship procedures. These experiences have provided a strong foundation for deploying more complex cloud projects in the future, while enhancing skills in managing, monitoring, and optimizing AWS infrastructure.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.1-week1/",
	"title": "Worklog – Week 1",
	"tags": [],
	"description": "",
	"content": "Objectives for Week 1 Get connected and familiarize myself with members of First Cloud Journey (FCJ). Understand basic AWS services and how to create and manage costs with an AWS account. Learn how to use the AWS Management Console and AWS CLI to interact with and manage services. Tasks Implemented This Week Day Task Description Start Date End Date References 2 - Get acquainted with FCJ members - Read and take note of internal rules and regulations at the internship unit 08/09/2025 08/09/2025 3 - Learn about AWS and its core service categories + Compute (EC2) + Storage (S3) + Networking (VPC) + Database (RDS) 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create an AWS Free Tier account - Learn about AWS Console and AWS CLI - Practice: + Create an AWS account + Manage identity and access permissions + Install and configure AWS CLI + Perform basic operations using AWS CLI 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn how to manage costs effectively with AWS Budgets + Budget + Cost Budget + Usage Budget + Reserved Instance (RI) Budget + Savings Plans Budget - Practice: + Create a Cost Budget + Create a Usage Budget + Create an RI Budget + Create a Savings Plans Budget + Clean up unused resources 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about AWS Support services - AWS Support plans + Basic, Developer, Business, and Enterprise plans - Types of support requests + Account and billing support + Service limit increase requests + Technical support - Practice: + Select the Basic support plan + Create a support request 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Outcomes of Week 1 Gained a solid understanding of AWS and its main service groups:\nCompute: Provides computing resources for applications such as virtual machines and containers. Storage: Used for data storage, backup, and recovery. Networking: Manages network infrastructure, security, and connectivity between AWS resources. Database: Provides managed relational and non-relational database services. Successfully created and configured an AWS Free Tier account.\nLearned how to create and manage IAM Groups and Users.\nUnderstood how to log in using IAM and how users within the same group share assigned permissions.\nBecame familiar with the AWS Management Console and learned how to search for, access, and use services through the web interface.\nInstalled and configured AWS CLI on a local machine, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nChecking account and configuration information Listing available regions Creating and deleting S3 buckets Using Amazon SNS Creating IAM groups and users and adding users to groups Creating and deleting access keys Creating and configuring a basic VPS Launching and terminating EC2 instances Learned how to manage and monitor AWS costs using:\nCreating and configuring different budget types (Cost, Usage, RI, Savings Plans) Cleaning up unused resources for effective cost management Understood AWS Support plans and how to create support requests through the Support Center:\nBasic: Free plan, supports account and billing-related issues via the Help Center. Developer: 29 USD/month, provides basic architectural guidance and unlimited technical support requests created by the root user. Business: 100 USD/month, commonly used by small and medium enterprises, includes use-case-based guidance, AWS Support API access, and unlimited support requests from all IAM users. Enterprise: 15,000 USD/month, designed for large enterprises with strict security standards, including comprehensive architectural and infrastructure support, strategic consulting, cost optimization, and prioritized support handling. Gained confidence in using both the AWS Management Console and AWS CLI for basic cloud operations.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.2-week2/",
	"title": "Worklog – Week 2",
	"tags": [],
	"description": "",
	"content": "Objectives for Week 2 Connect and get acquainted with members of First Cloud Journey (FCJ). Understand basic AWS services and how to use the AWS Management Console and AWS CLI. Tasks Implemented This Week Day Task Description Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of the internship unit’s rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its main service categories + Compute + Storage + Networking + Database + Others 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create an AWS Free Tier account - Learn about AWS Management Console \u0026amp; AWS CLI - Practice: + Create an AWS account + Install \u0026amp; configure AWS CLI + Learn how to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2 concepts: + Instance types + AMI + EBS + Others - Learn SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect to EC2 via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Outcomes of Week 2 Understood what AWS is and gained knowledge of the basic service groups:\nCompute Storage Networking Database Other supporting services Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to search for, access, and use AWS services through the web interface.\nInstalled and configured AWS CLI on the local machine, including:\nAccess Key Secret Key Default Region Other basic configurations Used AWS CLI to perform basic operations such as:\nChecking account and configuration information Retrieving the list of available regions Accessing and viewing EC2 services Creating and managing key pairs Checking information about running services Acquired the ability to use both the AWS web interface and AWS CLI in parallel for managing cloud resources.\nSuccessfully launched and connected to an EC2 instance via SSH.\nLearned how to attach and manage EBS volumes for EC2 instances.\nGained a clearer understanding of the relationship between EC2, AMI, EBS, and Elastic IP in building and operating cloud servers.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.3-week3/",
	"title": "Worklog – Week 3",
	"tags": [],
	"description": "",
	"content": "Study Orientation for Week 3 Gain an in-depth understanding of Amazon EC2 virtual server service – the core foundation of AWS infrastructure. Learn how to operate, monitor, and scale server systems based on real-world deployment models. Weekly Activities Day Main Activities Start Date End Date References 2 Study an overview of the EC2 service and related components such as: instance types, AMI, key pairs, EBS, instance store, user data, metadata, auto scaling, EFS/FSx, Lightsail, and MGN 11/08/2025 11/08/2025 3 Practice launching EC2 instances on Windows and Linux, creating snapshots for data backup, installing applications on EC2, managing resources using tags and resource groups, applying access control with IAM, and cleaning up unused resources 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn and configure Amazon CloudWatch: monitoring metrics, collecting logs, setting alarms, and building dashboards to track system status 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Study the EC2 Auto Scaling model, scaling mechanisms, launch templates, and Elastic Load Balancer; practice deploying a system with automatic scaling in and out 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Explore Amazon Lightsail, deploy application and database instances, configure load balancers, create snapshot backups, and upgrade instance configurations 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 3 During the third week, I focused on gaining in-depth knowledge of the EC2 service, which is a key component in deploying server infrastructure on AWS. Through both theoretical study and hands-on practice, I gained a clearer understanding of EC2 architecture as well as how to select appropriate instance configurations for different usage requirements. I was able to distinguish between common instance families such as general-purpose, compute-optimized, memory-optimized, and storage-optimized instances.\nIn addition, I understood the roles of AMI, key pairs, EBS, and instance store in the process of launching and operating virtual servers. The use of user data and metadata also helped me automate the environment setup process immediately when EC2 instances are initialized. I further explored EFS and FSx to serve the needs of shared storage and data access across multiple servers.\nFrom a practical perspective, I successfully deployed and connected to EC2 servers on both Windows and Linux operating systems. I learned how to create snapshots to back up data and restore it when necessary, as well as how to install basic web applications for testing purposes. By using tags and resource groups, I was able to manage cloud resources in a more structured and systematic manner. At the same time, through IAM, I learned how to restrict access permissions to ensure system security.\nIn terms of monitoring, I studied and applied Amazon CloudWatch to track system operations. I configured EC2 monitoring metrics, collected application logs, created alarms when CPU usage or network traffic exceeded defined thresholds, and built consolidated dashboards to visually monitor the overall system status.\nRegarding scalability, I gained a clear understanding of how EC2 Auto Scaling works, including manual scaling, scheduled scaling, and automatic scaling based on real-time workloads. I also successfully deployed an Auto Scaling Group integrated with a Load Balancer to ensure that the system can automatically adjust the number of running instances based on traffic fluctuations.\nAt the end of the week, I further explored Amazon Lightsail, a simplified platform for rapid deployment of common applications. I practiced deploying several models such as a WordPress website, a PrestaShop e-commerce system, and the Akaunting accounting software. In addition, I used Lightsail Database for data storage, Lightsail Load Balancer for traffic distribution, created snapshots for backup purposes, and performed instance configuration upgrades when required.\nThrough the activities completed in Week 3, I gained a solid understanding of the process of deploying, operating, monitoring, and scaling server systems on AWS. This has provided me with a strong foundation to continue learning and implementing real-world cloud systems in the following stages.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.4-week4/",
	"title": "Worklog – Week 4",
	"tags": [],
	"description": "",
	"content": "Study Orientation for Week 4 Explore AWS data storage services, with a primary focus on Amazon S3 and hybrid storage solutions. Understand backup, recovery, and disaster recovery strategies for system protection. Practice deploying highly available, secure, and scalable storage architectures. Weekly Activities Day Main Activities Start Date End Date References 2 Study Amazon S3 in detail: buckets, objects, access points, storage classes, versioning, CORS, access control, static website hosting, CloudFront integration, replication, and performance optimization; practice creating buckets, uploading data, deploying static websites, and replicating data across regions 11/08/2025 11/08/2025 3 Study the architecture of AWS Backup, design backup plans, configure SNS notifications; practice creating Backup Plans, monitoring backup activities, and testing data recovery 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Study the Snow Family, Storage Gateway (File, Volume, and Tape Gateway), and Disaster Recovery concepts (RTO, RPO); practice creating a File Gateway, sharing data, and connecting with on-premise environments 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Study AWS Import/Export; practice importing virtual machines from VMware to AWS and exporting EC2 instances from AWS to on-premise environments 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Study Amazon FSx; practice deploying Multi-AZ file server systems with SSD and HDD, performance testing, monitoring, snapshot configuration, capacity scaling, and throughput optimization 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 4 During the fourth week, I focused on studying AWS storage solutions as well as backup and recovery mechanisms to ensure system safety and reliability.\nFirst, I gained a solid understanding of how Amazon S3 works, including bucket and object structures, access points, and access control mechanisms using ACLs, bucket policies, CORS, and versioning. I successfully practiced creating S3 buckets, uploading data, deploying static websites directly on S3, and integrating CloudFront to accelerate content delivery. In addition, I implemented cross-region replication to enhance data availability and durability.\nAlongside S3, I explored hybrid storage solutions through AWS Storage Gateway, including File Gateway, Volume Gateway, and Tape Gateway. Connecting on-premise file shares to AWS helped me better understand real-world hybrid storage architectures. I also studied the Snow Family and common enterprise use cases for large-scale data transfer.\nRegarding backup and recovery, I researched the architecture of AWS Backup, the process of building Backup Plans, the role of Backup Vaults, and recovery workflows. I successfully configured automated backup systems, integrated SNS notifications for monitoring, and tested data restoration procedures. At the same time, I gained a clear understanding of Disaster Recovery strategies such as RTO, RPO, Pilot Light, Warm Standby, and Active-Active to ensure continuous system operation during failures.\nFor file-based storage systems, I practiced deploying Amazon FSx by building Multi-AZ architectures with both SSD and HDD storage. I created file shares, monitored performance, enabled data deduplication and shadow copy mechanisms, and managed user sessions. In addition, I performed storage capacity and throughput scaling to meet increasing usage demands.\nFurthermore, I practiced the Import/Export process between on-premise environments and AWS. Specifically, I exported virtual machines from VMware, uploaded them to S3, imported them as AMIs, and launched EC2 instances. Conversely, I also exported EC2 instances back to the local virtualization environment.\nThrough the activities completed in Week 4, I gained a comprehensive understanding of AWS storage, backup, recovery, and disaster recovery solutions. This knowledge serves as a critical foundation for designing highly available, secure, and scalable real-world cloud systems.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.5-week5/",
	"title": "Worklog – Week 5",
	"tags": [],
	"description": "",
	"content": "Study Orientation for Week 5 Gain an overview of the AWS security model and its core security principles. Understand identity management, authorization, and authentication mechanisms on the AWS platform. Practice encryption, monitoring, and data protection mechanisms in the cloud environment. Weekly Activities Day Main Activities Start Date End Date References 2 Study the Shared Responsibility Model and the division of responsibilities between AWS and customers; research an overview of Amazon IAM including Root Account, IAM User, IAM Group, IAM Policy, IAM Role, and Permission Boundary 11/08/2025 11/08/2025 3 Practice creating IAM Groups, IAM Users, IAM Roles, and Assume Roles; configure administrative users for EC2 and RDS; apply role conditions based on IP and access time; create restrictive policies and test access denial scenarios 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Study Tags and AWS Resource Groups; practice tagging EC2 instances via both Console and CLI; create Resource Groups; control EC2 access based on Tags using IAM Policies and test access denial scenarios 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn about Amazon Cognito, AWS Organizations, AWS Identity Center (SSO), AWS KMS, and AWS Security Hub; activate Security Hub and evaluate built-in security standards 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Practice AWS SSO in a multi-account model; create Organizational Units (OU) in AWS Organizations; encrypt data using AWS KMS; upload encrypted data to S3; enable CloudTrail and use Athena to query logs; test encrypted data sharing 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 5 During the fifth week, I focused on gaining in-depth knowledge of security architecture and identity management on the AWS platform. First, I fully understood the Shared Responsibility Model, in which AWS is responsible for securing the physical infrastructure, hardware, and foundational services, while customers are responsible for securing data, applications, access control, and encryption configurations. This helped me clearly distinguish the boundaries of responsibility when operating systems in the cloud environment.\nNext, I conducted a detailed study of Amazon Identity and Access Management (IAM). I gained a strong understanding of core concepts such as Root Account, IAM Users, IAM Groups, IAM Policies, IAM Roles, and Permission Boundaries. Through hands-on practice, I created multiple user groups, configured administrative accounts for services such as EC2 and RDS, and applied the principle of least privilege to minimize security risks. In addition, I configured Assume Roles with restrictions based on IP addresses and access time, and conducted testing on access-denied scenarios when security conditions were not satisfied.\nAlongside IAM, I studied and practiced using Tags for both resource management and access control. I attached Tags to EC2 resources using both the AWS Management Console and AWS CLI, created Resource Groups to organize resources by tags, and defined IAM Policies to allow or deny access based on Tag values and Regions. Testing scenarios involving denied access due to incorrect Region, incorrect Tags, or unmet conditions helped me better understand how AWS enforces fine-grained access control at the resource level.\nIn application user management, I explored Amazon Cognito, including User Pools for account management and authentication, and Identity Pools for granting temporary access to AWS services. At the same time, I studied multi-account operations using AWS Organizations and AWS Identity Center (SSO). I practiced creating Organizational Units (OUs), separating environments, and configuring centralized authentication via SSO, which helped me understand centralized identity management in multi-account architectures.\nRegarding data security, I learned how to use AWS Key Management Service (KMS) to create and manage encryption keys. I applied encryption to data stored in Amazon S3, then tested uploading, accessing, and sharing encrypted data through permissions granted by IAM and Roles. In parallel, I enabled AWS CloudTrail to log all account activities and used Amazon Athena to query logs for monitoring and security auditing purposes.\nIn addition, I activated and used AWS Security Hub to aggregate and evaluate default security standards. Through security reports and alerts, I gained insights into common misconfigurations and learned how to improve the overall security posture of the system.\nThrough the activities completed in Week 5, I developed a comprehensive understanding of the AWS security ecosystem, from identity management, access control, and resource tagging to data encryption, activity monitoring, and security compliance evaluation. This knowledge forms a critical foundation for deploying secure and production-ready cloud systems in real-world environments.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.6-week6/",
	"title": "Worklog – Week 6",
	"tags": [],
	"description": "",
	"content": "Learning Objectives for Week 6 Understand relational (SQL) and non-relational (NoSQL) database services on AWS. Learn how to deploy, operate, and back up databases securely in the cloud environment. Get familiar with query acceleration mechanisms using in-memory caching with Amazon ElastiCache. Practice interacting with AWS databases programmatically using the Python SDK. Weekly Implementation Details Day Main Activities Start Date End Date References 2 Study the overview of Amazon RDS and Amazon Aurora; database engines such as MySQL and PostgreSQL; Multi-AZ architecture; Read Replicas mechanism; automated backup and recovery 11/08/2025 11/08/2025 3 Hands-on practice with RDS deployment: preparing infrastructure including VPC, EC2, Security Group, and DB Subnet Group; deploying EC2 and RDS in Private Subnets; connecting EC2 to RDS; deploying applications; configuring automated backup and cleaning up resources 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn about Amazon DynamoDB (NoSQL): table structure, Primary Key, Sort Key, Read/Write Capacity models (On-demand, Provisioned); hands-on practice with table creation, data write/read/update/query operations; creating Global Secondary Index and querying through indexes 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn AWS SDK for Python including Botocore and Boto3; configure AWS CLI; use Python to interact with DynamoDB: create tables, insert, read, update, delete data, scan tables, load sample data, and remove all resources 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Learn Amazon ElastiCache for Redis: cluster, node, shard concepts; create Access Keys and configure AWS CLI; create ElastiCache Cluster via Console and CLI; use AWS SDK to perform read/write operations with string, hash, publish/subscribe, and stream 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 6 During the sixth week, I focused on studying and practicing with database systems and data storage services on the AWS platform. First, I researched Amazon RDS and Amazon Aurora, gaining a solid understanding of popular database engines such as MySQL and PostgreSQL, as well as how AWS ensures high availability through the Multi-AZ architecture. In addition, I learned about the Read Replicas mechanism used to improve read performance and system scalability, along with automated backup and recovery mechanisms to ensure data safety.\nIn the RDS hands-on session, I successfully deployed a complete system consisting of EC2 and RDS in a Private Subnet environment. I configured Security Groups and DB Subnet Groups appropriately to ensure secure communication between the application and the database. I also performed automated backups and tested data recovery procedures, helping me better understand real-world cloud database operations.\nNext, I studied Amazon DynamoDB – the NoSQL database service provided by AWS. I learned about table structures, the roles of Partition Key and Sort Key, and the principles of capacity allocation under both On-demand and Provisioned modes. Through hands-on practice, I created database tables, performed write, read, update, and query operations, and built Global Secondary Indexes to optimize queries on alternative attributes.\nAt the same time, I became familiar with the AWS SDK for Python ecosystem, including Botocore and Boto3. I configured AWS CLI for account access and used Python scripts to interact directly with DynamoDB, such as creating tables, inserting data, querying, scanning entire tables, and deleting resources. This programming-based interaction helped me better understand how real-world applications communicate with AWS services through APIs.\nIn addition, I explored and practiced with Amazon ElastiCache for Redis – AWS’s distributed in-memory caching service. I created an ElastiCache Cluster, connected to it via the AWS SDK, and performed read/write operations such as setting and getting strings, working with hashes, publish/subscribe messaging, and streams. Through this process, I clearly understood the important role of caching in accelerating data access and reducing the load on primary databases.\nBy the end of Week 6, I had gained a comprehensive understanding of AWS database models, including relational databases, NoSQL systems, and caching services. Moreover, I developed practical skills in deployment, programming, backup, and performance optimization, which form a critical foundation for building and operating cloud-based application systems.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.7-week7/",
	"title": "Worklog – Week 7",
	"tags": [],
	"description": "",
	"content": "Learning Objectives for Week 7 Get familiar with the Serverless architecture using AWS Lambda and Amazon API Gateway. Understand the working principles of Event-Driven systems on the AWS platform. Learn how to build asynchronous processing systems using Amazon SQS and SNS. Practice building APIs that connect Lambda with DynamoDB and S3 for data processing. Weekly Implementation Details Day Main Activities Start Date End Date References 2 Study AWS Lambda: function concepts and permission management using IAM Roles; learn Amazon API Gateway including HTTP methods and CORS mechanism; practice creating IAM Roles for Lambda, building Lambda functions for file upload processing, configuring API Gateway to trigger Lambda, and storing data in DynamoDB 11/08/2025 11/08/2025 3 Practice integrating Lambda with S3 and DynamoDB: building image processing Lambda functions, creating S3 Buckets, configuring IAM Policies for Lambda, creating and managing DynamoDB tables, building data-write Lambda functions, and cleaning up resources 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Practice deploying a static website with S3: creating buckets, enabling static hosting, attaching bucket policies, and uploading frontend files; building a CRUD system with DynamoDB and Lambda, configuring API Gateway, testing APIs using Postman and frontend, and cleaning up resources 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn Event-Driven architecture with Amazon SQS and SNS: publish/subscribe model and message queue principles; practice creating SQS Queues and SNS Topics, building Lambda and APIs to interact with SQS and SNS, and testing system operations 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Learn AWS Step Functions and their role in orchestrating microservices in Serverless systems 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 7 During the seventh week, I focused on studying and practicing Serverless and Event-Driven architecture models on the AWS platform. First, I explored how AWS Lambda works, gaining an understanding of how serverless functions are deployed without the need to manage servers, as well as the role of IAM Roles in granting resource access to Lambda functions based on the principle of least privilege.\nAt the same time, I learned about Amazon API Gateway, including how to build RESTful APIs, use HTTP methods, and configure CORS to allow secure access from frontend applications. Through hands-on practice, I successfully built a system where API Gateway acted as the entry point, triggering Lambda functions to process requests and store data in DynamoDB.\nNext, I practiced integrating Lambda with Amazon S3 and DynamoDB. I built Lambda functions to handle file uploads to S3 while storing metadata in DynamoDB. Through this workflow, I gained a clearer understanding of how to build a complete serverless backend system with three core components: API Gateway – Lambda – DynamoDB.\nIn addition, I successfully deployed a static website using Amazon S3, where the frontend interacted with backend APIs via API Gateway to perform create, read, update, and delete (CRUD) operations on data. Testing through Postman and directly on the web interface helped me better understand real-world request processing flows.\nAlongside Serverless architecture, I continued learning the Event-Driven model using Amazon SQS and SNS. I created SQS Queues and SNS Topics, built Lambda functions to process messages from queues, and practiced publish/subscribe mechanisms to simulate asynchronous processing systems. This helped me understand the role of message queues in decoupling system components, improving reliability, and enhancing scalability.\nToward the end of the week, I gained an overview of AWS Step Functions and their capability to orchestrate Lambda functions within microservices systems according to logical execution flows. Through this week’s learning activities, I established a solid foundation in Serverless and Event-Driven architectures, as well as modern backend system development on AWS with scalability, cost efficiency, and maintainability.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.8-week8/",
	"title": "Worklog – Week 8",
	"tags": [],
	"description": "",
	"content": "Objectives of Week 8 Understand and implement automated AWS resource provisioning using CloudFormation and AWS CDK. Manage system configurations and monitor sessions using AWS Systems Manager. Learn how to connect infrastructure components (EC2, Lambda, API Gateway, S3) using the Infrastructure as Code (IaC) approach. Weekly Implementation Details Day Activities Start Date End Date References 2 Learn AWS CloudFormation and Cloud9: template structure in JSON/YAML, Stack concept, and Drift Detection; Practice: create a basic CloudFormation template using Cloud9 11/08/2025 11/08/2025 3 Advanced CloudFormation Practice: deploy Lambda functions, create Stacks, connect EC2 resources, map resources using StackSets, perform Drift Detection checks, and clean up resources 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn AWS Systems Manager (SSM): Patch Manager, Run Command, Session Manager; Practice: create EC2 instances, attach IAM Roles, configure Patch Manager and Run Command, monitor sessions, and clean up resources 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn AWS CDK with VS Code: deploy public EC2, configure development environment, create ECS cluster and application, API Gateway + Load Balancer, Lambda, and S3; deploy Stack and Nested Stack, and clean up resources 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Session Manager Practice: create private and public EC2 instances, attach IAM Roles, connect from public to private instances, update IAM Roles for S3 access, create S3 buckets and S3 Gateway Endpoints, monitor session logs, configure Port Forwarding, and clean up resources 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 8 During this week, I focused on studying and practicing Infrastructure as Code (IaC) solutions and system configuration management:\nAWS CloudFormation \u0026amp; AWS CDK Gained a solid understanding of how to create, deploy, and manage Stacks, Nested Stacks, and StackSets. Practiced building JSON/YAML templates to automatically provision Lambda, EC2, S3, and API Gateway resources. Applied CloudFormation Drift Detection to identify differences between templates and actual deployed resources. Used AWS CDK with VS Code to deploy infrastructure in a more flexible, maintainable, and scalable manner. AWS Systems Manager (SSM) Managed EC2 instances remotely using Session Manager without direct SSH access. Configured Patch Manager and Run Command to automate system updates and EC2 operations. Monitored session logs and used Port Forwarding to securely access private EC2 instances. Automated Infrastructure Management Connected infrastructure components (Lambda, API Gateway, EC2, S3) using the IaC architecture. Understood safe workflows for deploying, updating, and deleting resources. Practiced cleaning up resources after labs to optimize costs and avoid unnecessary charges. Additional Skills Became familiar with Cloud9 for developing and editing CloudFormation and CDK templates. Understood the principles of monitoring and managing private/public EC2 sessions. Gained hands-on experience in deploying Serverless and containerized backend systems using API Gateway and Load Balancer. Through the activities in Week 8, I significantly strengthened my skills in automating AWS infrastructure, system management, and monitoring, providing a strong foundation for deploying more complex cloud environments in subsequent weeks.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.9-week9/",
	"title": "Worklog – Week 9",
	"tags": [],
	"description": "",
	"content": "Objectives of Week 9 Get familiar with the data ecosystem and Machine Learning (ML) services on AWS. Understand AWS AI/ML services and learn how to deploy data analysis pipelines. Learn to use pre-built AI services for data processing, recognition, analysis, and prediction. Weekly Implementation Details Day Activities Start Date End Date References 2 Learn AWS Glue (Crawler), Amazon Athena, Amazon QuickSight, Amazon SageMaker 11/08/2025 11/08/2025 3 Data Analysis Practice: create IAM Role \u0026amp; Policy, set up S3 Bucket, deploy Glue Crawler, create notebooks in Glue Studio, analyze data with Athena, visualize with QuickSight, clean up resources 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 SageMaker Practice: create SageMaker Studio, prepare dataset, analyze and export data to S3, train and tune ML models, deploy and evaluate performance, perform automated hyperparameter tuning, clean up resources 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn Amazon Bedrock: Foundation Models, Bedrock Agents, Knowledge Bases, Bedrock Inference Features 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Explore Pre-trained AI Services: Rekognition, Translate, Textract, Transcribe, Polly, Comprehend, Kendra, Lookout, Personalize 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 9 This week focused on the AWS data ecosystem and Machine Learning, combining both theoretical learning and hands-on practice with pre-built AI services:\nAWS Glue, Athena, and QuickSight Understood how Glue Crawler automatically discovers and catalogs data from S3. Used Athena to query S3 data directly using SQL. Visualized data with QuickSight, creating dynamic reports and dashboards. Amazon SageMaker Created and used SageMaker Studio to prepare datasets, train, and tune ML models. Learned the workflow from raw data to model training, evaluation, and exporting results. Practiced deploying models and testing performance, including automated hyperparameter tuning. Amazon Bedrock and Pre-trained AI Services Learned to use Foundation Models, Bedrock Agents, and Knowledge Bases. Understood Bedrock inference features for rapid AI deployment. Explored pre-built AI services: Rekognition: image and video recognition. Translate \u0026amp; Transcribe: translation and speech-to-text. Textract \u0026amp; Comprehend: data extraction and language analysis. Polly: text-to-speech synthesis. Kendra \u0026amp; Lookout: information search and anomaly detection. Personalize: personalized recommendation systems. Additional Skills Managed IAM Roles, Policies, and access rights for secure ML data handling. Connected data across S3, Glue, SageMaker, and QuickSight to build complete data pipelines. Cleaned up resources after practice sessions to optimize costs. Through Week 9, I gained the ability to deploy data and ML pipelines on AWS, understand how to integrate services for analysis, visualization, and ML model deployment, and become familiar with pre-built AI services to accelerate intelligent application development.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/2-proposal/",
	"title": "APT Magic – Proposal",
	"tags": [],
	"description": "",
	"content": "APT Magic A Serverless AI Platform for Personalized Image Generation and Social Interaction 1. Executive Summary APT Magic is a serverless AI-powered web application designed to enable users to generate, personalize, and share artistic content such as AI-generated images. The platform integrates with AI foundation models via Amazon Bedrock and provides a seamless web experience using Next.js (SSR) hosted on AWS Amplify.\nThe MVP version focuses on real-time image generation and sharing, while the Future Design aims to scale with SageMaker Inference, Step Functions, and AWS MLOps pipelines for advanced model orchestration and automation.\nAPT Magic is currently developed as a modern, cost-efficient, and secure AWS-native architecture for small to medium user bases, with planned expansion into enterprise-grade AI orchestration.\n2. Problem Statement What’s the Problem? Most AI image generation platforms are costly, rely on opaque third-party APIs, and offer limited personalization.\nDevelopers and creators often face high latency, lack of transparent model management, and limited control over user data security.\nThe Solution APT Magic leverages AWS serverless architecture to deliver:\nReal-time AI image generation through Amazon Bedrock Stability AI models. Secure user authentication and content management using Amazon Cognito and DynamoDB. Scalable API handling via AWS Lambda and API Gateway. Low-latency global delivery with CloudFront CDN and WAF protection. Future upgrades will include Step Functions orchestration, SQS/SNS decoupling, SageMaker Inference pipelines, and cost-efficient CI/CD via CodeBuild, CodePipeline, and CloudFormation. transforming APT Magic into a fully automated MLOps platform.\n3. Solution Architecture MVP Architecture The MVP is a fully serverless architecture, focusing on scalability, maintainability, and cost-effectiveness.\nCore AWS Services:\nRoute53 + CloudFront + WAF — Secure global access and caching. Amplify (Next.js SSR) — Hosts the frontend and server-side rendering layer. API Gateway + Lambda Functions — Manage backend logic (image processing, subscription, post APIs). Amazon Cognito — User authentication and access control. Amazon S3 + DynamoDB — Data persistence and image storage. Amazon Bedrock — Integrates foundation model (Stability AI) for image generation. Secrets Manager, CloudWatch, CloudTrail — Security, logging, and monitoring. Security\nPrivateLink for secure communication between Lambda and backend services. WAF + IAM policies for traffic filtering and role-based access control. Future Design (Enhanced Architecture) In the next phase, APT Magic will evolve into an AI orchestration platform, introducing new layers for automation, resilience, and model lifecycle management.\nNew Services to be Added:\nAWS Step Functions — To orchestrate asynchronous workflows such as:\nMulti-step AI image generation (prompt validation → inference → result upload). Payment confirmation → model processing → notification. Amazon SQS — For reliable message queuing between async Lambda tasks.\nAmazon SNS — For real-time event notifications to users or administrators.\nAmazon ElastiCache (Redis) — For rate limiting and caching of frequent inference requests.\nAmazon SageMaker Inference — For hosting custom fine-tuned models and managing model endpoints.\nAWS CodePipeline + SageMaker Pipelines — To automate MLOps: model training, evaluation, and deployment.\nAWS PrivateLink + VPC Endpoints — For secure data flow between Lambda, S3, and SageMaker.\nAWS WAF \u0026amp; Shield Advanced — For DDoS protection and advanced security filtering.\nCI/CD + MLOps\nCodePipeline + CodeBuild + CloudFormation for infrastructure deployment and automation. 4. Technical Implementation Implementation Phases Phase 1 – MVP Deployment (Completed / Current)\nImplement Amplify (Next.js SSR) + API Gateway + Lambda. Integrate Bedrock Stability AI API. Deploy CI/CD via CodePipeline + CloudFormation. Enable user authentication (Cognito) and storage (S3 + DynamoDB). Phase 2 – Future Design Expansion\nIntroduce Step Functions + SQS/SNS to manage async AI workflows. Add ElastiCache for request throttling and caching. Integrate SageMaker Inference for fine-tuned model hosting. Implement SageMaker Pipelines for automated training and deployment. Extend security with Shield Advanced + GuardDuty + PrivateLink. Connect GitLab Runner with CodeBuild for unified CI/CD. 5. Timeline \u0026amp; Milestones 6. Budget Estimation (AWS Pricing Estimate) Service Estimated Monthly Cost Notes Lambda + API Gateway $0.50 \u0026lt; 1M invocations Amplify (Next.js SSR) $0.35 Web hosting and build minutes S3 + DynamoDB $0.20 Image and metadata storage Bedrock Inference $3.00 Based on model usage (Stability AI) ElastiCache (Future) $1.00 Rate limit cache Step Functions + SQS/SNS $0.60 Workflow orchestration SageMaker Inference (Future) $5.00 Managed endpoint cost CloudWatch + WAF + Shield $1.00 Logging and protection Total (Est.) ~$11.65/month Scalable by usage 7. Risk Assessment Risk Impact Probability Mitigation AI model inference latency Medium High Use ElastiCache + Step Functions for async handling Cost increase from model calls High Medium Bedrock usage control, SageMaker autoscaling CI/CD misconfigurations Medium Low CloudFormation rollback policies Security vulnerabilities High Medium WAF, GuardDuty, PrivateLink, IAM least privilege Third-party API dependency Medium Medium Bedrock fallback to S3-stored inference results 8. Expected Outcomes Technical Outcomes: Complete serverless AI image generation workflow with secure CI/CD. Modular orchestration enabling rapid MLOps integration. Improved latency and reliability via caching and async workflows. Long-Term Value: A foundation for AI as a Service (AIaaS) platform expansion. Ready-to-scale MLOps framework with automated retraining. Reusable cloud infrastructure for future AI products. "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.2-prerequiste/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": " IAM Permissions Required Permissions AdministratorAccess AmazonBedrockFullAccess AWSCodeBuildAdminAccess AWSCodeBuildDeveloperAccess BedrockAgentCoreFullAccess Create a User and Assign Permissions Go to IAM → Users → select Create user. Add the permissions listed above. Complete the user creation and save the Access Key if you need it for the SDK. Download AWS CLI Download AWS CLI: AWS CLI Link\nThen install it following the instructions.\nUV Management Setup 1. Why use UV? UV is fast, lightweight, and manages environments better than pip.\n2. Install UV on Windows Run:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Add UV to PATH:\n$env:Path = \u0026#34;C:\\Users\\leamo\\.local\\bin;$env:Path\u0026#34; Restart your machine to apply the new PATH.\n3. Initialize a UV Environment Inside your project directory:\nuv init Then select the environment in VS Code.\nConnect Your Machine to AWS CLI Go back to IAM to create an Access Key.\nCreate Access Key and Configure AWS CLI In the user page: Security credentials → Create access key Choose Command Line Interface (CLI) Configure AWS CLI Run:\naws configure Fill in:\nAWS Access Key ID AWS Secret Access Key Default region name (example: ap-southeast-1) Default output format json Start AWS CLI AgentCore Run:\nuv run which agentcore After running, it will download all necessary libraries for AWS AgentCore.\nCreate Groq API Go to Groq and create an API key as shown. These external tools support RAG and are integrated through AWS AgentCore.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Review and consolidate knowledge of Serverless architecture and AI services. Design a detailed DynamoDB database schema for the project and its APIs. Thoroughly examine and validate the integration flow among system services. Tasks Implemented During the Week: Day Tasks Start Date Completion Date Reference 2 - Review and design the DynamoDB database + Revisit Partition Key (PK) and Sort Key (SK) concepts - Hands-on: + Design schemas for project tables + Define access patterns (data query strategies) 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Review and design the API layer (API Gateway + Lambda): + Revisit REST API creation and Lambda Proxy Integration + List required API endpoints such as POST and GET 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Review the AI integration workflow with Amazon Bedrock + Revisit Bedrock API invocation methods + Review fundamental Prompt Engineering techniques 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Review security and authentication mechanisms (Amazon Cognito + IAM): + Revisit the authentication flow: User login → Token issuance → Token submission to API Gateway + Review IAM policies based on the principle of least privilege 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Review system monitoring and operational practices + Revisit CloudWatch for planning runtime monitoring + Review the use of Boto3 SDK + Prepare a detailed project deployment plan 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Finalized the DynamoDB schema design:\nBuilt a well-structured data model optimized for efficient data access. Clearly defined service interfaces:\nSpecified clear input and output formats for Lambda functions. Gained strong proficiency in using the Boto3 library to interconnect AWS services. Prepared a prompt strategy for AI testing and validation with Amazon Bedrock.\nDeveloped a clear understanding of the identity and authentication flow from Amazon Cognito through API Gateway to AWS Lambda.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.10-week10/",
	"title": "Worklog – Week 10",
	"tags": [],
	"description": "",
	"content": "Objectives of Week 10 Understand and implement system monitoring using Amazon CloudWatch. Learn how AWS CloudTrail records API activities and manages logs in real time. Monitor serverless applications and frontend/backend systems with AWS Amplify. Weekly Implementation Details Day Activities Start Date End Date References 2 Learn Amazon CloudWatch: Metrics, Logs, Alarms, Events, Dashboards, AWS X-Ray 11/08/2025 11/08/2025 3 CloudWatch Practice: create IAM Role \u0026amp; Policy, configure EC2, set up CloudWatch Metrics \u0026amp; Logs, create Alarms, build Dashboards, clean up resources 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn AWS CloudTrail: Trails, Events (Read/Write/All), CloudTrail Insights 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn AWS Amplify: Frontend, Backend, Storage, Authentication 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Practice: Monitor Lambda with CloudWatch \u0026amp; X-Ray, host source code on Amplify, create custom metrics and Alarms, debug logs, monitor with X-Ray, clean up resources 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 10 This week focused on system monitoring and activity management on AWS, combining hands-on practice with CloudWatch, CloudTrail, and Amplify:\nAmazon CloudWatch Learned to use Metrics to monitor resources such as EC2, Lambda, and DynamoDB. Configured Logs for detailed tracking of application and infrastructure activities. Created Alarms to notify when thresholds are breached or issues occur. Designed Dashboards for an overview of AWS resources. Utilized X-Ray for debugging and performance analysis of Lambda functions and API Gateway. AWS CloudTrail Learned how to record all API calls and monitor Read/Write events. Used CloudTrail Insights to detect unusual activities within the system. Integrated with CloudWatch to generate alerts and automate responses for critical events. AWS Amplify Quickly hosted frontend and backend applications. Implemented authentication and storage to integrate with Lambda and DynamoDB. Monitored application performance via CloudWatch Logs and X-Ray. Additional Skills Managed IAM Roles \u0026amp; Policies for proper security and access control. Combined CLI and Console for simultaneous management of multiple AWS resources. Planned and executed resource cleanup after practice to optimize costs. Through Week 10, I developed the ability to monitor AWS systems in real-time, debug serverless applications, create alerts and dashboards, and efficiently manage frontend/backend resources using Amplify.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/1-worklog/1.11-week11/",
	"title": "Worklog – Week 11",
	"tags": [],
	"description": "",
	"content": "Objectives of Week 11 Understand the CI/CD workflow and AWS tools supporting it. Explore and gain hands-on experience with Amazon pre-trained AI services. Weekly Implementation Details Day Activities Start Date End Date References 2 Participate in event: Generative AI with Amazon Bedrock + Update knowledge on Foundation Models on Amazon Bedrock + Learn more about pre-trained AI services 11/08/2025 11/08/2025 3 Learn AWS CI/CD services: + AWS CodeCommit + AWS CodeBuild + AWS CodeDeploy + AWS CodePipeline 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn and practice AWS Elastic Beanstalk: + Application \u0026amp; Environment setup + Create CloudFormation Stack + Connect EC2 instances and configure database + Deploy and update applications on Elastic Beanstalk + Monitor environment status and query EC2 server information + Clean up resources 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 Amazon Polly Practice: + Configure DynamoDB + Explore Text-to-Speech (TTS) features + Generate speech and speech marks using CLI and Java SDK 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 Amazon Rekognition Practice: + Create Cognito Identity Pool + Detect objects and recognize faces + Test “find a person” application + Attach EBS volumes to EC2 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Summary of Achievements in Week 11 This week focused on CI/CD deployment, Elastic Beanstalk, and hands-on experience with AWS AI services, with the following highlights:\nAWS CI/CD Understood automated software development processes using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline. Learned how to manage source code, build, test, and deploy applications automatically. AWS Elastic Beanstalk Deployed web applications quickly on a managed environment. Created and updated stacks using CloudFormation, connected EC2 instances and databases. Monitored environment health and queried server information. Amazon AI Services Amazon Polly: Convert text to speech, generate speech marks, integrate with DynamoDB. Amazon Rekognition: Detect objects, recognize faces, and test “find a person” application. Configured Cognito Identity Pool to manage user access permissions. Additional Skills Managed IAM Roles \u0026amp; Policies to grant proper permissions for services. Used both Console and CLI for managing, testing, and cleaning up AWS resources. Learned to combine CI/CD, compute, and AI services to deploy end-to-end applications. Through Week 11, I developed the ability to deploy web applications using CI/CD, explore pre-trained AI services, and manage the full pipeline from code to deployment, while also practicing monitoring and securing AWS resources.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.3-architecture/5.3.4-agent-core/",
	"title": "5.3.2. Chunking &amp; Embedding",
	"tags": [],
	"description": "",
	"content": " Why Chunking Matters Documents and FAQs are often long; to generate embeddings effectively and optimize similarity search, large text needs to be split into smaller segments (chunks).\nBenefits Reduce loss of context during embedding More accurate retrieval using vector similarity Better performance during search Avoid token limit issues of embedding models Chunking Strategy The code uses RecursiveCharacterTextSplitter:\nsplitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) Chunking Parameters Parameter Value Meaning chunk_size 500 Size of each chunk (characters) chunk_overlap 0 No overlap between chunks Optimization Suggestions Recommended chunk size: 500–1000 tokens (depends on the embedding model)\nChunk overlap: If you need continuous context, set overlap to 50–100 characters\nTrade-off:\nSmall chunks → higher accuracy but more vectors Large chunks → fewer vectors but potential context loss Creating Embeddings \u0026amp; Vector Store Initialize the Embedding Model emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) Chosen model: all-MiniLM-L6-v2\nLightweight and fast Good for English Embedding size: 384 dimensions Create FAISS Vector Store faq_store = FAISS.from_documents(chunks, emb) FAISS (Facebook AI Similarity Search) provides:\nFast vector search Efficient for large datasets Supports multiple index algorithms Querying the Vector Store results = faq_store.similarity_search(query, k=3) Parameters:\nquery: User question k=3: Returns top 3 most similar chunks Important Notes Data Updates If your data changes (add/update documents), you need to:\nRe-embed all documents, or Incrementally update the vector store Choosing an Embedding Model Criteria Lightweight Model Heavy Model Speed Fast Slow Accuracy Good Excellent Cost Low High Use cases FAQ, chatbot Research, legal, complex RAG Vietnamese Models If you need better Vietnamese support:\nkeepitreal/vietnamese-sbert sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 Full Code Example from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.vectorstores import FAISS # Load documents docs = load_faq_csv() # Chunking splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) # Embedding emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) # Vector Store faq_store = FAISS.from_documents(chunks, emb) # Query query = \u0026#34;How do I change my password?\u0026#34; results = faq_store.similarity_search(query, k=3) Implementation Checklist Prepare documents (CSV, JSON, text files) Choose chunk_size (test 500, 750, 1000) Select embedding model (English or multilingual) Build and save FAISS index Test retrieval with sample queries Monitor and adjust k value "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.3-architecture/5.3.1-agentcore-memory/",
	"title": "AgentCore Memory",
	"tags": [],
	"description": "",
	"content": "Configuring Memory in AgentCore To enable memory for AgentCore, follow the steps below.\n1. Create Memory in Bedrock Go to Bedrock → select AgentCore. Switch to the Memory tab. Click Create memory. In the memory creation interface, you will see the following sections:\nMemory name Set the name for the memory that AgentCore will use.\nShort-term memory (raw event) expiration The number of days detailed conversation history is stored. For this demo, you can keep the default 90 days.\n2. Types of Memory in AgentCore 1. Summarization – Conversation Summaries Function: Summarizes the conversation after it ends or periodically. Purpose: Keeps long-term context while using minimal storage.\nExample: You have 100 messages about AWS CLI errors → later the Agent remembers:\n“The user was experiencing AWS CLI connection issues.”\n2. Semantic Memory Function: Stores key facts or knowledge independent of context. Purpose: Used to answer questions based on previously mentioned information.\nExample:\n“Project A uses Python 3.9.” Ask again later → Agent responds with Python 3.9 immediately.\n3. User Preferences Function: Learns user habits and communication style. Purpose: Personalizes responses.\nExample: If you often say:\n“Keep the answer short.” The Agent will consistently respond concisely.\n4. Episodes Function: Stores sequences of events and analyzes success/failure through Reflections. Purpose: Helps the Agent learn from past experiences.\nExample: A previous flight booking failed due to missing dates → the Agent remembers. Next time, it asks for the date first.\n3. Memory Type Used in the Demo For the demo, you only need:\nSummarization Choose Summarization and click Create to complete the setup.\n4. Update Memory ID in Python After creating a Memory, you will receive a Memory ID.\nAdd it to your Python file:\n# AgentCore Memory Configuration REGION = \u0026#34;ap-southeast-1\u0026#34; MEMORY_ID = \u0026#34;memory_j98zj-4LFDxqB2o1\u0026#34; GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) Be sure to update the Memory ID and Region to match your configuration.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.3-architecture/5.3.2-groq-api/",
	"title": "Calling Groq API",
	"tags": [],
	"description": "",
	"content": " Objective Use the Groq library (ChatGroq / init_chat_model with model_provider=\u0026quot;groq\u0026quot;) to call an OpenAI-compatible model hosted on Groq.\nConfiguration in Code In the demo code:\nRetrieve API Key from Environment GROQ_API_KEY = os.getenv(\u0026#34;GROQ_API_KEY\u0026#34;) The variable GROQ_API_KEY loads the API key from the environment variable.\nInitialize the Model llm = init_chat_model( model=\u0026#34;openai/gpt-oss-20b\u0026#34;, model_provider=\u0026#34;groq\u0026#34;, api_key=GROQ_API_KEY ) Integrate with Agent The Agent calls the LLM through create_agent(...) using the model=llm parameter:\nagent = create_agent( model=llm, tools=tools, checkpointer=checkpointer, store=store, middleware=[MemoryMiddleware()], system_prompt=system_prompt, ) Processing Flow Agent → Groq API → Model Inference → Response "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.3-architecture/5.3.3-chunking/",
	"title": "Chunking &amp; Embedding",
	"tags": [],
	"description": "",
	"content": " Why Chunking Is Needed Documents or FAQs are often long; to compute embeddings effectively and optimize similarity search, large text must be split into smaller segments (chunks).\nBenefits Reduces context loss during embedding More accurate retrieval with vector similarity Better performance for search Compatible with token limits of embedding models Chunking Strategy The code uses RecursiveCharacterTextSplitter:\nsplitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) Chunking Parameters Parameter Value Meaning chunk_size 500 Size of each chunk (characters) chunk_overlap 0 No overlap between chunks Optimization Tips Recommended chunk size: 500–1000 tokens (depends on embedding model)\nChunk overlap: Use 50–100 characters if continuous context is needed\nTrade-off:\nSmaller chunks → higher accuracy but more vectors Larger chunks → fewer vectors but may lose context Creating Embeddings and Vector Store Initialize Embedding Model emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) Selected model: all-MiniLM-L6-v2\nLightweight and fast Good for English Embedding dimension: 384 Create FAISS Vector Store faq_store = FAISS.from_documents(chunks, emb) FAISS (Facebook AI Similarity Search) provides:\nHigh-speed vector search Efficient handling of large datasets Multiple index algorithms Query the Vector Store results = faq_store.similarity_search(query, k=3) Parameters:\nquery: User question k=3: Returns top 3 most relevant chunks Important Notes Updating Data If documents change (add/update):\nRe-embed all data, or Apply incremental updates to the vector store Choosing an Embedding Model Trade-off comparison:\nCriteria Lightweight Model Heavy Model Speed Fast Slow Accuracy Good Very good Cost Low High Use cases FAQ, chatbot Research, legal Vietnamese Models For better Vietnamese performance:\nkeepitreal/vietnamese-sbert sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 Full Code Example from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.vectorstores import FAISS # Load documents docs = load_faq_csv() # Chunking splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=0 ) chunks = splitter.split_documents(docs) # Embedding emb = HuggingFaceEmbeddings( model_name=\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ) # Vector Store faq_store = FAISS.from_documents(chunks, emb) # Query query = \u0026#34;How do I change my password?\u0026#34; results = faq_store.similarity_search(query, k=3) Deployment Checklist Prepare documents (CSV, JSON, text files) Select appropriate chunk_size (test 500, 750, 1000) Choose embedding model (English or multilingual) Build and save FAISS index Test retrieval with sample queries Monitor and tune the k value for similarity search "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.3-architecture/",
	"title": "RAG Architecture Deployed on AWS AgentCore",
	"tags": [],
	"description": "",
	"content": " Using the Gateway Endpoint In this section, we will explore how to integrate Groq to call OpenAI-compatible models and how to perform data chunking for RAG.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/5.4-agent-core-run/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day\nDate \u0026amp; Time: 9:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Generative AI with Amazon Bedrock\nDate \u0026amp; Time: 08:30, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building a RAG Agent with Groq API and AgentCore Memory Overview In this workshop, we will build a complete RAG (Retrieval-Augmented Generation) Agent with the following capabilities:\nCalling the Groq API to use high-performance LLM models Chunking \u0026amp; Embedding documents for optimized vector search AgentCore Memory to maintain long-term context across chat sessions Tool Integration so the agent can automatically search FAQs and reformulate queries AgentCore provides a framework for building AI agents with memory persistence, middleware hooks, and tool orchestration — enabling the agent to “remember” conversation history and personalize responses.\nContents Workshop Overview\nPrerequisites\nArchitecture\n5.3.1. Calling Groq API 5.3.2. Chunking \u0026amp; Embedding 5.3.3. AgentCore Code Handler Running AgentCore\nResource Cleanup\nTech Stack Component Technology LLM Provider Groq API (OpenAI models) Embedding Model HuggingFace (all-MiniLM-L6-v2) Vector Store FAISS Agent Framework LangChain + AgentCore Memory Backend AgentCore Memory Store Text Splitting RecursiveCharacterTextSplitter Prerequisites Python 3.8+ Groq API Key Basic understanding of RAG and LLMs Familiarity with vector embeddings Expected Outcome By the end of the workshop, you will have:\nAn agent capable of answering FAQs via vector search A memory system that remembers user preferences and context Tool orchestration allowing the agent to decide when to use tools Production-ready code with logging and error handling Get Started: 5.1. Workshop Overview\n"
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/6-self-evaluation/",
	"title": "Self-Evaluation",
	"tags": [],
	"description": "",
	"content": "During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired at school in a real work environment.\nI participated in [brief description of main project or tasks], which helped me improve my skills in [list skills: programming, analysis, report writing, communication…].\nRegarding my work style, I always strive to complete tasks on time, follow the rules, and actively communicate with colleagues to enhance work efficiency.\nBelow is my self-assessment based on key criteria:\nNo. Criteria Description Excellent Good Average 1 Professional Knowledge \u0026amp; Skills Solid understanding of the field, practical application, and effective use of tools ✅ ☐ ☐ 2 Learning Ability Quickly and effectively acquires new knowledge ✅ ☐ ☐ 3 Proactivity Initiates tasks and solves problems without waiting for guidance ✅ ☐ ☐ 4 Responsibility Completes tasks on time with quality and reliability ✅ ☐ ☐ 5 Discipline Adheres to working hours, rules, and processes ☐ ✅ ☐ 6 Progressiveness Willing to receive feedback and improve performance and personal development ☐ ✅ ☐ 7 Communication Clearly presents ideas, reports work, and exchanges information effectively ☐ ✅ ☐ 8 Teamwork Works effectively in a team and collaborates well with colleagues ✅ ☐ ☐ 9 Professional Conduct Respects colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-Solving Skills Identifies problems and proposes creative and practical solutions ☐ ✅ ☐ 11 Contribution to Projects/Organization Effectively contributes to projects, proposes improvements, recognized by the team ✅ ☐ ☐ 12 Overall Overall evaluation of the internship experience ✅ ☐ ☐ Areas for Improvement Strengthen discipline and strictly follow company or organizational rules. Enhance problem-solving and critical thinking skills. Improve communication in daily work and handling practical situations. "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/7-feedback/",
	"title": "Sharing &amp; Feedback",
	"tags": [],
	"description": "",
	"content": " Here you can freely share your personal experiences participating in the First Cloud Journey program and provide feedback to help the FCJ team improve any shortcomings in the following areas:\nGeneral Feedback 1. Work Environment\nThe work environment is friendly and open. FCJ team members are always ready to support when difficulties arise, even outside working hours. The workspace is tidy and comfortable, which helps me focus better. It could be enhanced with more team bonding or networking activities to help everyone understand each other better.\n2. Mentor / Team Admin Support\nMentors provide detailed guidance, clear explanations when something is unclear, and always encourage questions. Team admins assist with procedures, documentation, and ensure smooth workflow. I highly appreciate that mentors allow me to try and resolve issues independently rather than just giving answers.\n3. Relevance to Field of Study\nThe tasks assigned match the knowledge I learned at school, while also expanding into areas I had never encountered before. This allowed me to strengthen foundational knowledge while acquiring practical skills.\n4. Learning Opportunities \u0026amp; Skill Development\nDuring the internship, I gained new skills such as using project management tools, teamwork, and professional communication in a corporate environment. Mentors also shared practical experiences, helping me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is positive: everyone respects each other, works seriously but remains friendly. During urgent projects, everyone collaborates and supports one another regardless of their role. This made me feel part of the team, even as an intern.\n6. Intern Policies / Benefits\nThe company provides internship allowances and flexible working hours when needed. Participating in internal training sessions is also a major plus.\nAdditional Questions What were you most satisfied with during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you advise them to intern here? Why? Suggestions \u0026amp; Wishes Do you have any suggestions to improve the internship experience? Would you like this program to continue in the future? Other feedback (feel free to share): "
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://xchilryhkang.github.io/AWS_Intern_Worklog/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]